"""
íƒì§€ ì„œë¹„ìŠ¤ - ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° ê°ì²´ íƒì§€
"""
import cv2
import numpy as np
from typing import List, Dict, Any, Optional
from .model_manager import ModelManager
from .memory_service import MemoryService
from config.settings import MAX_VIDEO_WIDTH, DENSE_ANALYSIS_FPS_RATIO, CROP_BATCH_SIZE

class DetectionService:
    """ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° ê°ì²´ íƒì§€ ì„œë¹„ìŠ¤"""
    
    def __init__(self, model_manager: ModelManager, memory_service: MemoryService):
        self.model_manager = model_manager
        self.memory_service = memory_service
        self.crop_batch_size = CROP_BATCH_SIZE
        
    def process_video(self, video_path: str, dense_analysis: bool = True) -> List[Dict[str, Any]]:
        """ìŠ¤ë§ˆíŠ¸ ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        detector = self.model_manager.get_detector()
        classifier = self.model_manager.get_classifier()
        
        if not detector or not detector.is_loaded():
            print("ERROR: Detector not loaded!")
            return []
        
        print("ğŸš€ Starting smart streaming video processing")
        print(f"ğŸ¯ Detector: {self.model_manager.detector_type}")
        print(f"ğŸ§  Classifier: {self.model_manager.classifier_type}")
        
        try:
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # ìŠ¤ë§ˆíŠ¸ í”„ë ˆì„ ê°„ê²© ê²°ì •
            if dense_analysis:
                frame_interval = max(1, int(fps // DENSE_ANALYSIS_FPS_RATIO))
            else:
                frame_interval = max(1, total_frames // 80)
            
            print(f"ğŸ“¹ Video: {total_frames} frames, {fps:.1f} fps")
            print(f"âš¡ Processing every {frame_interval} frames")
            
            annotations = []
            frame_count = 0
            processed_frames = 0
            crop_batch = []
            crop_metadata = []
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_count % frame_interval == 0:
                    frame_time = frame_count / fps
                    print(f"\nğŸ¬ Frame {processed_frames + 1} (time: {frame_time:.3f}s)")
                    
                    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í”„ë ˆì„ í¬ê¸° ì¡°ì •
                    frame_resized, scale = self._resize_frame(frame)
                    
                    # ê°ì²´ íƒì§€
                    detections = detector.detect(frame_resized)
                    print(f"ğŸ¯ {self.model_manager.detector_type} found: {len(detections)} detections")
                    
                    if detections:
                        # ë¶„ë¥˜ê¸° ì‚¬ìš© ì—¬ë¶€ ê²°ì •
                        use_classifier = self._should_use_classifier(len(detections), processed_frames)
                        print(f"ğŸ§  Classifier usage: {'YES' if use_classifier else 'NO'} (memory: {self.memory_service.get_current_usage():.1f}MB)")
                        
                        if use_classifier and classifier and classifier.is_loaded():
                            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ í¬ë¡­ ìˆ˜ì§‘
                            for detection in detections:
                                bbox = detection['bbox']
                                
                                # ì›ë³¸ í¬ê¸°ë¡œ bbox ë³µì›
                                bbox_original = [int(x / scale) for x in bbox] if scale != 1.0 else bbox
                                
                                # í¬ë¡­ ì´ë¯¸ì§€ ì¤€ë¹„
                                x, y, w, h = bbox
                                cropped = frame_resized[y:y+h, x:x+w]
                                
                                if cropped.size > 0:
                                    crop_batch.append(cropped)
                                    crop_metadata.append({
                                        'detection': detection,
                                        'bbox_original': bbox_original,
                                        'frame_time': frame_time,
                                        'frame': frame
                                    })
                            
                            # ë°°ì¹˜ê°€ ì°¼ê±°ë‚˜ ë§ˆì§€ë§‰ì´ë©´ ë¶„ë¥˜ê¸° ì²˜ë¦¬
                            if len(crop_batch) >= self.crop_batch_size or frame_count == total_frames - 1:
                                enhanced_detections = self._process_classifier_batch(crop_batch, crop_metadata)
                                annotations.extend(enhanced_detections)
                                
                                # ë°°ì¹˜ ì´ˆê¸°í™”
                                crop_batch = []
                                crop_metadata = []
                                
                                # ë©”ëª¨ë¦¬ ì •ë¦¬
                                self.memory_service.optimize_memory()
                        else:
                            # ë¶„ë¥˜ê¸° ì—†ì´ íƒì§€ ê²°ê³¼ë§Œ ì‚¬ìš©
                            for detection in detections:
                                bbox = detection['bbox']
                                if scale != 1.0:
                                    bbox = [int(x / scale) for x in bbox]
                                
                                annotation = {
                                    'frame': f"{frame_time:.3f}",
                                    'label': detection['class_name'],
                                    'bbox': bbox,
                                    'confidence': detection['confidence'],
                                    'original_class': detection['class_name'],
                                    'enhanced_by_classifier': False,
                                    'source': 'auto'
                                }
                                annotations.append(annotation)
                    
                    processed_frames += 1
                    
                    # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
                    if processed_frames % 10 == 0:
                        current_memory = self.memory_service.get_current_usage()
                        print(f"ğŸ§¹ Memory cleanup - Current: {current_memory:.1f}MB")
                        self.memory_service.optimize_memory()
                
                frame_count += 1
            
            cap.release()
            
            # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
            if crop_batch:
                enhanced_detections = self._process_classifier_batch(crop_batch, crop_metadata)
                annotations.extend(enhanced_detections)
            
            print(f"\nâœ… Smart processing completed!")
            print(f"ğŸ“Š Total detections: {len(annotations)}")
            print(f"ğŸ§  Final memory usage: {self.memory_service.get_current_usage():.1f}MB")
            
            return annotations
            
        except Exception as e:
            print(f"âŒ Smart processing error: {e}")
            return []
    
    def _resize_frame(self, frame: np.ndarray) -> tuple[np.ndarray, float]:
        """í”„ë ˆì„ í¬ê¸° ì¡°ì •"""
        original_height, original_width = frame.shape[:2]
        if original_width > MAX_VIDEO_WIDTH:
            scale = MAX_VIDEO_WIDTH / original_width
            new_width = int(original_width * scale)
            new_height = int(original_height * scale)
            frame_resized = cv2.resize(frame, (new_width, new_height))
            print(f"ğŸ“ Resized: {original_width}x{original_height} â†’ {new_width}x{new_height}")
            return frame_resized, scale
        else:
            return frame.copy(), 1.0
    
    def _should_use_classifier(self, detection_count: int, processed_frames: int) -> bool:
        """ë¶„ë¥˜ê¸° ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        memory_usage = self.memory_service.get_current_usage()
        
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ê²°ì •
        if memory_usage < 3000:  # 3GBê¹Œì§€ëŠ” ë¬´ì¡°ê±´ ì‚¬ìš©
            return True
        elif memory_usage < 4000:  # 4GBê¹Œì§€ëŠ” ì„ íƒì  ì‚¬ìš©
            return processed_frames % 2 == 0
        else:
            return processed_frames % 4 == 0
    
    def _process_classifier_batch(self, crop_batch: List[np.ndarray], crop_metadata: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë¶„ë¥˜ê¸° ë°°ì¹˜ ì²˜ë¦¬"""
        classifier = self.model_manager.get_classifier()
        if not crop_batch or not classifier:
            return []
        
        print(f"ğŸ§  Processing classifier batch: {len(crop_batch)} crops")
        enhanced_detections = []
        
        try:
            # ë°°ì¹˜ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            batch_size = min(4, len(crop_batch))
            
            for i in range(0, len(crop_batch), batch_size):
                mini_batch = crop_batch[i:i+batch_size]
                mini_metadata = crop_metadata[i:i+batch_size]
                
                # íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ë¥˜
                for cropped, metadata in zip(mini_batch, mini_metadata):
                    detection = metadata['detection']
                    frame = metadata['frame']
                    bbox = [0, 0, cropped.shape[1], cropped.shape[0]]  # í¬ë¡­ëœ ì´ë¯¸ì§€ ì „ì²´
                    
                    # ë¶„ë¥˜ê¸°ë¡œ íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ë¥˜
                    features = classifier.extract_features(frame, metadata['detection']['bbox'])
                    if features is not None:
                        enhanced_label = classifier.classify_features(features, detection['class_name'])
                    else:
                        enhanced_label = detection['class_name']
                    
                    annotation = {
                        'frame': f"{metadata['frame_time']:.3f}",
                        'label': enhanced_label,
                        'bbox': metadata['bbox_original'],
                        'confidence': detection['confidence'],
                        'original_class': detection['class_name'],
                        'enhanced_by_classifier': features is not None,
                        'source': 'auto'
                    }
                    enhanced_detections.append(annotation)
                
                # ë¯¸ë‹ˆ ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                del mini_batch
                self.memory_service.optimize_memory()
        
        except Exception as e:
            print(f"âš ï¸ Classifier batch processing error: {e}")
            # ì—ëŸ¬ ì‹œ íƒì§€ ê²°ê³¼ë§Œ ë°˜í™˜
            for metadata in crop_metadata:
                detection = metadata['detection']
                annotation = {
                    'frame': f"{metadata['frame_time']:.3f}",
                    'label': detection['class_name'],
                    'bbox': metadata['bbox_original'],
                    'confidence': detection['confidence'],
                    'original_class': detection['class_name'],
                    'enhanced_by_classifier': False,
                    'source': 'auto'
                }
                enhanced_detections.append(annotation)
        
        return enhanced_detections
