from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import cv2
import torch
import numpy as np
from ultralytics import YOLO
import requests
from PIL import Image
import io
import base64
import os
import tempfile
import json
from pathlib import Path
import random
import colorsys
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import psutil
import gc
from collections import deque

app = Flask(__name__)
CORS(app)  # CORS ì„¤ì •ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì™€ í†µì‹  í—ˆìš©

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì €ì¥
yolo_model = None
dinov2_model = None
dinov2_processor = None

# ì—…ë¡œë“œ í´ë” ì„¤ì •
UPLOAD_FOLDER = 'uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
training_data_accumulator = []  # ëˆ„ì  í•™ìŠµ ë°ì´í„°
current_custom_model_path = None  # í˜„ì¬ ë¡œë“œëœ ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ

class UniversalCustomLabelClassifier:
    """DINOv2ë¥¼ í™œìš©í•œ ë²”ìš© ì»¤ìŠ¤í…€ ë¼ë²¨ ë¶„ë¥˜ê¸°"""
    
    def __init__(self):
        self.label_features = {}  # ë¼ë²¨ë³„ íŠ¹ì§• ë°ì´í„°ë² ì´ìŠ¤
        self.feature_clusters = {}  # ë¼ë²¨ë³„ í´ëŸ¬ìŠ¤í„°
        self.similarity_threshold = 0.7  # ìœ ì‚¬ë„ ì„ê³„ê°’
        self.min_samples_for_clustering = 3  # í´ëŸ¬ìŠ¤í„°ë§ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        
    def learn_from_manual_annotations(self, video_path, manual_annotations, dinov2_analyzer):
        """ìˆ˜ë™ ë¼ë²¨ë§ ë°ì´í„°ì—ì„œ íŒ¨í„´ í•™ìŠµ"""
        print("ğŸ§  DINOv2 íŒ¨í„´ í•™ìŠµ ì‹œì‘...")
        
        if not manual_annotations:
            print("âš ï¸ ìˆ˜ë™ ë¼ë²¨ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            learned_labels = set()
            
            for ann in manual_annotations:
                label = ann['label']
                frame_time = float(ann['frame'])
                bbox = ann['bbox']
                
                # í•´ë‹¹ í”„ë ˆì„ìœ¼ë¡œ ì´ë™
                frame_number = int(frame_time * fps)
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
                ret, frame = cap.read()
                
                if not ret:
                    continue
                
                # DINOv2 íŠ¹ì§• ì¶”ì¶œ
                features = dinov2_analyzer.analyze_with_dinov2(frame, bbox)
                
                if features is not None:
                    if label not in self.label_features:
                        self.label_features[label] = []
                    
                    self.label_features[label].append(features.flatten())
                    learned_labels.add(label)
            
            cap.release()
            
            # ê° ë¼ë²¨ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            for label in learned_labels:
                self._create_label_clusters(label)
            
            print(f"âœ… í•™ìŠµ ì™„ë£Œ: {len(learned_labels)}ê°œ ë¼ë²¨ ({sum(len(features) for features in self.label_features.values())}ê°œ ìƒ˜í”Œ)")
            return True
            
        except Exception as e:
            print(f"âŒ íŒ¨í„´ í•™ìŠµ ì˜¤ë¥˜: {e}")
            return False
    
    def _create_label_clusters(self, label):
        """ë¼ë²¨ë³„ íŠ¹ì§• í´ëŸ¬ìŠ¤í„° ìƒì„±"""
        if label not in self.label_features or len(self.label_features[label]) < self.min_samples_for_clustering:
            print(f"ğŸ“Š {label}: ìƒ˜í”Œ ë¶€ì¡±ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ê±´ë„ˆëœ€ ({len(self.label_features.get(label, []))}ê°œ)")
            return
        
        try:
            features = np.array(self.label_features[label])
            
            # ì ì‘ì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
            n_samples = len(features)
            n_clusters = min(3, max(1, n_samples // 2))  # ìµœëŒ€ 3ê°œ í´ëŸ¬ìŠ¤í„°
            
            if n_clusters > 1:
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
                cluster_labels = kmeans.fit_predict(features)
                
                self.feature_clusters[label] = {
                    'kmeans': kmeans,
                    'centroids': kmeans.cluster_centers_,
                    'labels': cluster_labels,
                    'features': features
                }
                
                print(f"ğŸ“Š {label}: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„° ìƒì„± ({n_samples}ê°œ ìƒ˜í”Œ)")
            else:
                # í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œë©´ í‰ê·  íŠ¹ì§•ë§Œ ì €ì¥
                self.feature_clusters[label] = {
                    'centroids': [np.mean(features, axis=0)],
                    'features': features
                }
                print(f"ğŸ“Š {label}: ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° ìƒì„± ({n_samples}ê°œ ìƒ˜í”Œ)")
                
        except Exception as e:
            print(f"âŒ {label} í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜: {e}")
    
    def classify_with_learned_patterns(self, features, base_class_name):
        """í•™ìŠµëœ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¸ë¶€ ë¶„ë¥˜"""
        if features is None:
            return base_class_name
        
        features_flat = features.flatten()
        best_label = base_class_name
        best_similarity = 0
        
        # ëª¨ë“  í•™ìŠµëœ ë¼ë²¨ì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
        for label, cluster_info in self.feature_clusters.items():
            if 'centroids' in cluster_info:
                for centroid in cluster_info['centroids']:
                    similarity = cosine_similarity([features_flat], [centroid])[0][0]
                    
                    if similarity > best_similarity and similarity > self.similarity_threshold:
                        best_similarity = similarity
                        best_label = label
        
        # ìœ ì‚¬í•œ íŒ¨í„´ì„ ì°¾ì•˜ìœ¼ë©´ ë” êµ¬ì²´ì ì¸ ë¼ë²¨ ì œì•ˆ
        if best_label != base_class_name and best_similarity > self.similarity_threshold:
            confidence = best_similarity * 100
            print(f"ğŸ¯ DINOv2 ì„¸ë¶€ ë¶„ë¥˜: {base_class_name} â†’ {best_label} (ì‹ ë¢°ë„: {confidence:.1f}%)")
            return best_label
        
        return base_class_name
    
    def get_learned_labels_info(self):
        """í•™ìŠµëœ ë¼ë²¨ ì •ë³´ ë°˜í™˜"""
        info = {}
        for label, features in self.label_features.items():
            cluster_count = len(self.feature_clusters.get(label, {}).get('centroids', []))
            info[label] = {
                'sample_count': len(features),
                'cluster_count': cluster_count,
                'has_clustering': cluster_count > 1
            }
        return info
    
    def save_learned_patterns(self, filepath):
        """í•™ìŠµëœ íŒ¨í„´ì„ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(filepath, 'wb') as f:
                pickle.dump({
                    'label_features': self.label_features,
                    'feature_clusters': self.feature_clusters,
                    'similarity_threshold': self.similarity_threshold
                }, f)
            return True
        except Exception as e:
            print(f"íŒ¨í„´ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def load_learned_patterns(self, filepath):
        """ì €ì¥ëœ íŒ¨í„´ì„ ë¡œë“œ"""
        try:
            with open(filepath, 'rb') as f:
                data = pickle.load(f)
                self.label_features = data['label_features']
                self.feature_clusters = data['feature_clusters']
                self.similarity_threshold = data['similarity_threshold']
            return True
        except Exception as e:
            print(f"íŒ¨í„´ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False

class AutoLabelingSystem:
    def __init__(self):
        self.yolo_model = None
        self.dinov2_model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.universal_classifier = UniversalCustomLabelClassifier()
        
        # ğŸš€ DINOv2 ìµœëŒ€ í™œìš©ì„ ìœ„í•œ ì ê·¹ì  ë©”ëª¨ë¦¬ ê´€ë¦¬
        total_memory = self.get_available_memory()
        self.memory_limit = total_memory * 0.85  # 85%ê¹Œì§€ ì ê·¹ í™œìš©
        self.dinov2_threshold = total_memory * 0.75  # 75%ê¹Œì§€ëŠ” DINOv2 ë¬´ì¡°ê±´ ì‚¬ìš©
        self.crop_batch_size = 6  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        
        print(f"ğŸš€ AGGRESSIVE DINOv2 mode enabled!")
        print(f"ğŸ“Š Total memory: {total_memory:.1f}MB")
        print(f"ğŸ“Š Memory limit: {self.memory_limit:.1f}MB")
        print(f"ğŸ“Š DINOv2 safe zone: {self.dinov2_threshold:.1f}MB")
        print(f"Using device: {self.device}")
    
    def get_available_memory(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ê³„ì‚° (MB)"""
        memory = psutil.virtual_memory()
        available_mb = memory.available / 1024 / 1024
        return available_mb
    
    def get_current_memory_usage(self):
        """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def should_use_dinov2(self, current_detections_count, processed_frames):
        """DINOv2 ì ê·¹ ì‚¬ìš© ë¡œì§"""
        memory_usage = self.get_current_memory_usage()
        
        # ğŸš€ ë©”ëª¨ë¦¬ ì œí•œì„ ëŒ€í­ ì™„í™”
        if memory_usage < 3000:  # 3GBê¹Œì§€ëŠ” ë¬´ì¡°ê±´ ì‚¬ìš©
            return True
        elif memory_usage < 4000:  # 4GBê¹Œì§€ëŠ” ì„ íƒì  ì‚¬ìš©
            return processed_frames % 2 == 0  # 2í”„ë ˆì„ë§ˆë‹¤
        else:
            return processed_frames % 4 == 0  # 4í”„ë ˆì„ë§ˆë‹¤
    
    def load_models(self):
        """YOLOv8ê³¼ DINOv2 ëª¨ë¸ ë¡œë“œ"""
        try:
            # YOLOv8 ëª¨ë¸ ë¡œë“œ (ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸)
            print("=" * 50)
            print("ğŸ¤– Starting AI model initialization...")
            print("=" * 50)
            
            print("ğŸ“¥ Loading YOLOv8 model...")
            self.yolo_model = YOLO('yolov8n.pt')  # nano ë²„ì „ (ë¹ ë¦„)
            print("âœ… YOLOv8 loaded successfully!")
            print(f"   Available classes: {len(self.yolo_model.names)} classes")
            print(f"   Device: {self.device}")
            
            # DINOv2 ëª¨ë¸ ë¡œë“œ (ì˜µì…”ë„)
            print("ğŸ“¥ Loading DINOv2 model...")
            try:
                # DINOv2 ëª¨ë¸ ë¡œë“œ (Hugging Face transformers ì‚¬ìš©)
                from transformers import AutoProcessor, AutoModel
                self.dinov2_processor = AutoProcessor.from_pretrained('facebook/dinov2-base')
                self.dinov2_model = AutoModel.from_pretrained('facebook/dinov2-base')
                self.dinov2_model = self.dinov2_model.to(self.device)
                print("âœ… DINOv2 (Hugging Face) loaded successfully!")
            except ImportError as ie:
                print(f"âš ï¸  Transformers not available ({ie}), trying alternative...")
                try:
                    # ëŒ€ì•ˆ: torch.hubë¥¼ í†µí•œ ë¡œë“œ
                    self.dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
                    if hasattr(self.dinov2_model, 'to'):
                        self.dinov2_model = self.dinov2_model.to(self.device)
                    self.dinov2_processor = None
                    print("âœ… DINOv2 (torch.hub) loaded successfully!")
                except Exception as te:
                    print(f"âš ï¸  DINOv2 loading failed: {te}")
                    print("ğŸ“ DINOv2 will be skipped - only YOLO detection will be used")
                    self.dinov2_model = None
                    self.dinov2_processor = None
            
            print("=" * 50)
            print("ğŸ‰ Model initialization completed!")
            print(f"âœ… YOLO: {'Loaded' if self.yolo_model else 'Failed'}")
            print(f"âœ… DINOv2: {'Loaded' if self.dinov2_model else 'Skipped'}")
            print("=" * 50)
                
        except Exception as e:
            print(f"âŒ Critical error loading models: {e}")
            print("ğŸ”§ Please check your dependencies and model files")
            return False
        return True
    
    def extract_frames(self, video_path, max_frames=None, frame_interval=1):
        """ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        cap = cv2.VideoCapture(video_path)
        frames = []
        frame_indices = []
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        print(f"Total frames: {total_frames}, FPS: {fps}")
        
        # max_framesê°€ Noneì´ë©´ ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬
        if max_frames is None:
            step = frame_interval  # ê¸°ë³¸ 1í”„ë ˆì„ë§ˆë‹¤
        else:
            step = max(frame_interval, total_frames // max_frames)
        
        print(f"Processing every {step} frames")
        
        frame_count = 0
        processed_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            if frame_count % step == 0:
                frames.append(frame)
                frame_indices.append(frame_count / fps)  # ì‹œê°„(ì´ˆ) ì €ì¥
                processed_count += 1
                
                # ë„ˆë¬´ ë§ì€ í”„ë ˆì„ ì²˜ë¦¬ ë°©ì§€ (ë©”ëª¨ë¦¬ ë³´í˜¸)
                if max_frames and processed_count >= max_frames:
                    break
                
            frame_count += 1
            
        cap.release()
        print(f"Extracted {len(frames)} frames for processing")
        return frames, frame_indices
    
    def detect_objects_yolo(self, frame):
        """YOLOv8ë¡œ ê°ì²´ íƒì§€"""
        if self.yolo_model is None:
            print("ERROR: YOLO model is not loaded!")
            return []
            
        try:
            results = self.yolo_model(frame, conf=0.15)  # ì‹ ë¢°ë„ ì„ê³„ê°’ì„ 0.15ë¡œ ëŒ€í­ ë‚®ì¶¤
            detections = []
            
            print(f"YOLO inference completed, processing {len(results)} results")
            
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    print(f"Found {len(boxes)} boxes in this result")
                    for box in boxes:
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        conf = box.conf[0].cpu().numpy()
                        cls = box.cls[0].cpu().numpy()
                        
                        # ë” ë§ì€ ê°ì²´ íƒì§€ë¥¼ ìœ„í•´ ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš©
                        if conf > 0.2:  # 0.3 -> 0.2ë¡œ ë” ë‚®ì¶¤
                            class_name = self.yolo_model.names[int(cls)]
                            detection = {
                                'bbox': [int(x1), int(y1), int(x2-x1), int(y2-y1)],  # x, y, w, h
                                'confidence': float(conf),
                                'class_id': int(cls),
                                'class_name': class_name
                            }
                            detections.append(detection)
                            print(f"Detected: {class_name} with confidence {conf:.3f}")
                else:
                    print("No boxes found in this result")
            
            print(f"Total detections after filtering: {len(detections)}")
            return detections
            
        except Exception as e:
            print(f"Error in YOLO detection: {e}")
            return []
    
    def analyze_with_dinov2(self, frame, bbox):
        """DINOv2ë¡œ í¬ë¡­ëœ ì´ë¯¸ì§€ ë¶„ì„"""
        x, y, w, h = bbox
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ í¬ë¡­
        cropped = frame[y:y+h, x:x+w]
        if cropped.size == 0:
            return None
            
        # BGRì„ RGBë¡œ ë³€í™˜
        cropped_rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(cropped_rgb)
        
        try:
            if self.dinov2_processor is not None:
                # Hugging Face transformers ì‚¬ìš©
                inputs = self.dinov2_processor(images=pil_image, return_tensors="pt")
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    if hasattr(self.dinov2_model, '__call__'):
                        outputs = self.dinov2_model(**inputs)
                        features = outputs.last_hidden_state.mean(dim=1)  # Global average pooling
                    else:
                        features = None
            else:
                # torch.hub ë²„ì „ ì‚¬ìš©
                try:
                    import torchvision.transforms as transforms
                    
                    # ì˜¬ë°”ë¥¸ transforms ì‚¬ìš©
                    transform = transforms.Compose([
                        transforms.ToPILImage(),
                        transforms.Resize((224, 224)),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                           std=[0.229, 0.224, 0.225])
                    ])
                    
                    # PIL ì´ë¯¸ì§€ë¥¼ numpy arrayë¡œ ë³€í™˜ í›„ tensorë¡œ
                    tensor_image = transform(pil_image).unsqueeze(0).to(self.device)
                    
                except ImportError:
                    print("torchvision not available, skipping DINOv2 analysis")
                    return None
                
                with torch.no_grad():
                    if hasattr(self.dinov2_model, '__call__'):
                        features = self.dinov2_model(tensor_image)
                    else:
                        features = None
            
            if features is not None and hasattr(features, 'cpu'):
                return features.cpu().numpy()
            else:
                return None
            
        except Exception as e:
            print(f"DINOv2 analysis error: {e}")
            return None
    
    def classify_with_features(self, features, class_name):
        """DINOv2 íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜ ê°œì„  - ë²”ìš© ë¶„ë¥˜ê¸° ì‚¬ìš©"""
        if hasattr(self, 'universal_classifier') and self.universal_classifier:
            return self.universal_classifier.classify_with_learned_patterns(features, class_name)
        else:
            # ê¸°ë³¸ ë¶„ë¥˜ê¸°ê°€ ì—†ìœ¼ë©´ ì›ë˜ í´ë˜ìŠ¤ ì´ë¦„ ë°˜í™˜
            return class_name
    
    def process_video_smart_streaming(self, video_path, dense_analysis=True):
        """ğŸš€ ìŠ¤ë§ˆíŠ¸ ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ì²˜ë¦¬ - YOLO + DINOv2 ìµœëŒ€ í™œìš©"""
        if not self.yolo_model:
            print("ERROR: Models not loaded!")
            return []
        
        print("ğŸš€ Starting SMART streaming video processing")
        print(f"ğŸ¯ Target: Maximum YOLO + DINOv2 synergy with memory efficiency")
        
        try:
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # ğŸ“Š ìŠ¤ë§ˆíŠ¸ í”„ë ˆì„ ê°„ê²© ê²°ì •
            if dense_analysis:
                frame_interval = max(1, int(fps // 4))  # ì´ˆë‹¹ 4í”„ë ˆì„
            else:
                frame_interval = max(1, total_frames // 80)  # ìµœëŒ€ 80í”„ë ˆì„
            
            print(f"ğŸ“¹ Video: {total_frames} frames, {fps:.1f} fps")
            print(f"âš¡ Processing every {frame_interval} frames")
            
            annotations = []
            frame_count = 0
            processed_frames = 0
            crop_batch = []  # DINOv2 ë°°ì¹˜ ì²˜ë¦¬ìš©
            crop_metadata = []  # í¬ë¡­ ë©”íƒ€ë°ì´í„°
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_count % frame_interval == 0:
                    frame_time = frame_count / fps
                    print(f"\nğŸ¬ Frame {processed_frames + 1} (time: {frame_time:.3f}s)")
                    
                    # ğŸ“ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í”„ë ˆì„ í¬ê¸° ì¡°ì •
                    original_height, original_width = frame.shape[:2]
                    if original_width > 1024:  # í¬ê¸° ì œí•œ
                        scale = 1024 / original_width
                        new_width = int(original_width * scale)
                        new_height = int(original_height * scale)
                        frame_resized = cv2.resize(frame, (new_width, new_height))
                        print(f"ğŸ“ Resized: {original_width}x{original_height} â†’ {new_width}x{new_height}")
                    else:
                        frame_resized = frame.copy()
                        scale = 1.0
                    
                    # ğŸ¯ YOLO ê°ì²´ íƒì§€
                    detections = self.detect_objects_yolo(frame_resized)
                    print(f"ğŸ¯ YOLO found: {len(detections)} detections")
                    
                    if detections:
                        # ğŸ§  DINOv2 ì‚¬ìš© ì—¬ë¶€ ìŠ¤ë§ˆíŠ¸ ê²°ì •
                        use_dinov2 = self.should_use_dinov2(len(detections), processed_frames)
                        print(f"ğŸ§  DINOv2 usage: {'YES' if use_dinov2 else 'NO'} (memory: {self.get_current_memory_usage():.1f}MB)")
                        
                        if use_dinov2 and self.dinov2_model:
                            # ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ í¬ë¡­ ìˆ˜ì§‘
                            for detection in detections:
                                bbox = detection['bbox']
                                
                                # ì›ë³¸ í¬ê¸°ë¡œ bbox ë³µì›
                                if scale != 1.0:
                                    bbox_original = [int(x / scale) for x in bbox]
                                else:
                                    bbox_original = bbox
                                
                                # í¬ë¡­ ì´ë¯¸ì§€ ì¤€ë¹„
                                x, y, w, h = bbox
                                cropped = frame_resized[y:y+h, x:x+w]
                                
                                if cropped.size > 0:
                                    crop_batch.append(cropped)
                                    crop_metadata.append({
                                        'detection': detection,
                                        'bbox_original': bbox_original,
                                        'frame_time': frame_time
                                    })
                            
                            # ğŸš€ ë°°ì¹˜ê°€ ì°¼ê±°ë‚˜ ë§ˆì§€ë§‰ì´ë©´ DINOv2 ì²˜ë¦¬
                            if len(crop_batch) >= self.crop_batch_size or frame_count == total_frames - 1:
                                enhanced_detections = self.process_dinov2_batch(crop_batch, crop_metadata)
                                annotations.extend(enhanced_detections)
                                
                                # ë°°ì¹˜ ì´ˆê¸°í™”
                                crop_batch = []
                                crop_metadata = []
                                
                                # ë©”ëª¨ë¦¬ ì •ë¦¬
                                self.optimize_memory()
                        else:
                            # DINOv2 ì—†ì´ YOLO ê²°ê³¼ë§Œ ì‚¬ìš©
                            for detection in detections:
                                bbox = detection['bbox']
                                if scale != 1.0:
                                    bbox = [int(x / scale) for x in bbox]
                                
                                annotation = {
                                    'frame': f"{frame_time:.3f}",
                                    'label': detection['class_name'],
                                    'bbox': bbox,
                                    'confidence': detection['confidence'],
                                    'original_class': detection['class_name'],
                                    'enhanced_by_dinov2': False
                                }
                                annotations.append(annotation)
                    
                    processed_frames += 1
                    
                    # ğŸ§¹ ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
                    if processed_frames % 10 == 0:
                        current_memory = self.get_current_memory_usage()
                        print(f"ğŸ§¹ Memory cleanup - Current: {current_memory:.1f}MB")
                        self.optimize_memory()
                
                frame_count += 1
            
            cap.release()
            
            # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
            if crop_batch:
                enhanced_detections = self.process_dinov2_batch(crop_batch, crop_metadata)
                annotations.extend(enhanced_detections)
            
            print(f"\nâœ… Smart processing completed!")
            print(f"ğŸ“Š Total detections: {len(annotations)}")
            print(f"ğŸ§  Final memory usage: {self.get_current_memory_usage():.1f}MB")
            
            return annotations
            
        except Exception as e:
            print(f"âŒ Smart processing error: {e}")
            return []
    
    def process_dinov2_batch(self, crop_batch, crop_metadata):
        """ğŸš€ DINOv2 ë°°ì¹˜ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì """
        if not crop_batch or not self.dinov2_model:
            return []
        
        print(f"ğŸ§  Processing DINOv2 batch: {len(crop_batch)} crops")
        enhanced_detections = []
        
        try:
            # ë°°ì¹˜ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            batch_size = min(4, len(crop_batch))
            
            for i in range(0, len(crop_batch), batch_size):
                mini_batch = crop_batch[i:i+batch_size]
                mini_metadata = crop_metadata[i:i+batch_size]
                
                # DINOv2 íŠ¹ì§• ì¶”ì¶œ
                features_batch = []
                for cropped in mini_batch:
                    # BGR to RGB
                    cropped_rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(cropped_rgb)
                    
                    # DINOv2 ë¶„ì„
                    features = self.analyze_with_dinov2_optimized(pil_image)
                    features_batch.append(features)
                
                # ê° í¬ë¡­ì— ëŒ€í•´ ê²°ê³¼ ìƒì„±
                for features, metadata in zip(features_batch, mini_metadata):
                    detection = metadata['detection']
                    
                    # DINOv2 ê¸°ë°˜ ì„¸ë¶€ ë¶„ë¥˜
                    if features is not None:
                        enhanced_label = self.classify_with_features(features, detection['class_name'])
                    else:
                        enhanced_label = detection['class_name']
                    
                    annotation = {
                        'frame': f"{metadata['frame_time']:.3f}",
                        'label': enhanced_label,
                        'bbox': metadata['bbox_original'],
                        'confidence': detection['confidence'],
                        'original_class': detection['class_name'],
                        'enhanced_by_dinov2': features is not None
                    }
                    enhanced_detections.append(annotation)
                
                # ë¯¸ë‹ˆ ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                del mini_batch, features_batch
                self.optimize_memory()
        
        except Exception as e:
            print(f"âš ï¸ DINOv2 batch processing error: {e}")
            # ì—ëŸ¬ ì‹œ YOLO ê²°ê³¼ë§Œ ë°˜í™˜
            for metadata in crop_metadata:
                detection = metadata['detection']
                annotation = {
                    'frame': f"{metadata['frame_time']:.3f}",
                    'label': detection['class_name'],
                    'bbox': metadata['bbox_original'],
                    'confidence': detection['confidence'],
                    'original_class': detection['class_name'],
                    'enhanced_by_dinov2': False
                }
                enhanced_detections.append(annotation)
        
        return enhanced_detections
    
    def analyze_with_dinov2_optimized(self, pil_image):
        """ìµœì í™”ëœ DINOv2 ë¶„ì„"""
        try:
            if self.dinov2_processor is not None:
                # Hugging Face transformers ì‚¬ìš©
                inputs = self.dinov2_processor(images=pil_image, return_tensors="pt")
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    if hasattr(self.dinov2_model, '__call__'):
                        outputs = self.dinov2_model(**inputs)
                        features = outputs.last_hidden_state.mean(dim=1)
                        return features.cpu().numpy()
            else:
                # torch.hub ë²„ì „ ì‚¬ìš©
                import torchvision.transforms as transforms
                
                transform = transforms.Compose([
                    transforms.Resize((224, 224)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                       std=[0.229, 0.224, 0.225])
                ])
                
                tensor_image = transform(pil_image).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    if hasattr(self.dinov2_model, '__call__'):
                        features = self.dinov2_model(tensor_image)
                        return features.cpu().numpy()
            
            return None
            
        except Exception as e:
            print(f"DINOv2 optimized analysis error: {e}")
            return None

    # ê¸°ì¡´ process_video í•¨ìˆ˜ë¥¼ êµì²´
    def process_video(self, video_path, dense_analysis=True):
        """ë©”ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ - ìŠ¤ë§ˆíŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©"""
        return self.process_video_smart_streaming(video_path, dense_analysis)
    
    def prepare_training_data(self, video_path, annotations, output_dir, accumulated_data=None):
        """ìˆ˜ë™ ë¼ë²¨ë§ ë°ì´í„°ë¥¼ YOLO í•™ìŠµ í¬ë§·ìœ¼ë¡œ ë³€í™˜ (ëˆ„ì  í•™ìŠµ ì§€ì›)"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # ì´ë¯¸ì§€ì™€ ë¼ë²¨ í´ë” ìƒì„±
            images_dir = os.path.join(output_dir, 'images')
            labels_dir = os.path.join(output_dir, 'labels')
            os.makedirs(images_dir, exist_ok=True)
            os.makedirs(labels_dir, exist_ok=True)
            
            cap = cv2.VideoCapture(video_path)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # í˜„ì¬ ìˆ˜ë™ ë¼ë²¨ë§ ë°ì´í„°
            current_manual_annotations = [ann for ann in annotations if ann.get('source') == 'manual' or ann.get('source') is None]
            
            # ëˆ„ì  ë°ì´í„°ì™€ í˜„ì¬ ë°ì´í„° ë³‘í•©
            all_manual_annotations = current_manual_annotations.copy()
            if accumulated_data:
                all_manual_annotations.extend(accumulated_data)
            
            # ê³ ìœ  ë¼ë²¨ ì¶”ì¶œ ë° í´ë˜ìŠ¤ ë§¤í•‘ ìƒì„±
            unique_labels = list(set([ann['label'] for ann in all_manual_annotations]))
            class_mapping = {label: i for i, label in enumerate(unique_labels)}
            
            print(f"Total training annotations: {len(all_manual_annotations)}")
            print(f"Classes: {unique_labels}")
            
            # classes.txt íŒŒì¼ ìƒì„±
            with open(os.path.join(output_dir, 'classes.txt'), 'w') as f:
                for label in unique_labels:
                    f.write(f"{label}\n")
            
            # í˜„ì¬ ë¹„ë””ì˜¤ì˜ ì–´ë…¸í…Œì´ì…˜ì„ ì‹œê°„ë³„ë¡œ ê·¸ë£¹í™”
            time_annotations = {}
            for ann in current_manual_annotations:
                frame_time = float(ann['frame'])
                if frame_time not in time_annotations:
                    time_annotations[frame_time] = []
                time_annotations[frame_time].append(ann)
            
            saved_frames = 0
            frame_count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                current_time = frame_count / fps
                
                # í˜„ì¬ ì‹œê°„ì— í•´ë‹¹í•˜ëŠ” ì–´ë…¸í…Œì´ì…˜ ì°¾ê¸°
                matching_annotations = []
                for time_key, anns in time_annotations.items():
                    if abs(time_key - current_time) < (1.0 / fps):
                        matching_annotations.extend(anns)
                
                if matching_annotations:
                    # í”„ë ˆì„ ì €ì¥
                    frame_filename = f"frame_{saved_frames:06d}.jpg"
                    frame_path = os.path.join(images_dir, frame_filename)
                    cv2.imwrite(frame_path, frame)
                    
                    # YOLO í¬ë§· ë¼ë²¨ íŒŒì¼ ìƒì„±
                    label_filename = f"frame_{saved_frames:06d}.txt"
                    label_path = os.path.join(labels_dir, label_filename)
                    
                    with open(label_path, 'w') as f:
                        for ann in matching_annotations:
                            x, y, w, h = ann['bbox']
                            label = ann['label']
                            
                            if label in class_mapping:
                                # YOLO í¬ë§·ìœ¼ë¡œ ë³€í™˜ (normalized coordinates)
                                x_center = (x + w/2) / width
                                y_center = (y + h/2) / height
                                norm_width = w / width
                                norm_height = h / height
                                
                                class_id = class_mapping[label]
                                f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\n")
                    
                    saved_frames += 1
                
                frame_count += 1
            
            cap.release()
            
            # ëˆ„ì  ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¶”ê°€ë¡œ ì²˜ë¦¬ (ê¸°ì¡´ ì´ë¯¸ì§€ ë³µì‚¬)
            if accumulated_data:
                self._copy_accumulated_data(accumulated_data, images_dir, labels_dir, class_mapping, saved_frames)
            
            # dataset.yaml íŒŒì¼ ìƒì„±
            yaml_content = f"""
path: {output_dir}
train: images
val: images

nc: {len(unique_labels)}
names: {unique_labels}
"""
            with open(os.path.join(output_dir, 'dataset.yaml'), 'w') as f:
                f.write(yaml_content)
            
            return {
                'success': True,
                'images_count': saved_frames,
                'total_annotations': len(all_manual_annotations),
                'classes': unique_labels,
                'dataset_path': os.path.join(output_dir, 'dataset.yaml'),
                'current_annotations': current_manual_annotations
            }
            
        except Exception as e:
            print(f"Training data preparation error: {e}")
            return {'success': False, 'error': str(e)}
    
    def _copy_accumulated_data(self, accumulated_data, images_dir, labels_dir, class_mapping, start_idx):
        """ëˆ„ì ëœ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒˆ í•™ìŠµ í´ë”ì— ë³µì‚¬"""
        try:
            for i, ann in enumerate(accumulated_data):
                if 'image_path' in ann and os.path.exists(ann['image_path']):
                    # ì´ë¯¸ì§€ ë³µì‚¬
                    new_image_name = f"accumulated_{start_idx + i:06d}.jpg"
                    new_image_path = os.path.join(images_dir, new_image_name)
                    import shutil
                    shutil.copy2(ann['image_path'], new_image_path)
                    
                    # ë¼ë²¨ íŒŒì¼ ìƒì„±
                    label_filename = f"accumulated_{start_idx + i:06d}.txt"
                    label_path = os.path.join(labels_dir, label_filename)
                    
                    with open(label_path, 'w') as f:
                        label = ann['label']
                        if label in class_mapping:
                            x, y, w, h = ann['bbox']
                            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì •ë³´ê°€ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í˜„ì¬ ë¹„ë””ì˜¤ í¬ê¸° ì‚¬ìš©
                            img_width = ann.get('image_width', 640)
                            img_height = ann.get('image_height', 480)
                            
                            x_center = (x + w/2) / img_width
                            y_center = (y + h/2) / img_height
                            norm_width = w / img_width
                            norm_height = h / img_height
                            
                            class_id = class_mapping[label]
                            f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\n")
        except Exception as e:
            print(f"Error copying accumulated data: {e}")
    
    def train_custom_model(self, dataset_path, epochs=50, imgsz=640):
        """ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ í›ˆë ¨ (ê°œì„ ëœ íŒŒë¼ë¯¸í„°)"""
        try:
            from ultralytics import YOLO
            
            print(f"ğŸš€ Enhanced training starting with {epochs} epochs...")
            
            # ë² ì´ìŠ¤ ëª¨ë¸ì—ì„œ ì‹œì‘
            model = YOLO('yolov8n.pt')
            
            # í•™ìŠµ ì‹¤í–‰ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¡°ì •)
            results = model.train(
                data=dataset_path,
                epochs=epochs,
                imgsz=imgsz,
                batch=8,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
                lr0=0.001,  # ì´ˆê¸° í•™ìŠµë¥  ì¡°ì •
                lrf=0.01,  # ìµœì¢… í•™ìŠµë¥ 
                momentum=0.937,  # ëª¨ë©˜í…€
                weight_decay=0.0005,  # ê°€ì¤‘ì¹˜ ê°ì‡ 
                warmup_epochs=3,  # ì›œì—… ì—í¬í¬
                warmup_momentum=0.8,  # ì›œì—… ëª¨ë©˜í…€
                patience=15,  # ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬ ì¦ê°€
                close_mosaic=10,  # ë§ˆì§€ë§‰ 10 ì—í¬í¬ì—ì„œ ëª¨ìì´í¬ ë¹„í™œì„±í™”
                mixup=0.1,  # MixUp ì¦ê°•
                copy_paste=0.1,  # Copy-Paste ì¦ê°•
                degrees=10.0,  # íšŒì „ ê°ë„
                translate=0.1,  # ì´ë™
                scale=0.9,  # ìŠ¤ì¼€ì¼ ë³€í™”
                shear=2.0,  # ì „ë‹¨ ë³€í™˜
                perspective=0.0001,  # ì›ê·¼ ë³€í™˜
                flipud=0.5,  # ìƒí•˜ ë’¤ì§‘ê¸°
                fliplr=0.5,  # ì¢Œìš° ë’¤ì§‘ê¸°
                mosaic=1.0,  # ëª¨ìì´í¬ ì¦ê°•
                save=True,
                project='custom_training',
                name='custom_model',
                exist_ok=True,
                pretrained=True,
                optimizer='AdamW',  # ë” ë‚˜ì€ ì˜µí‹°ë§ˆì´ì €
                verbose=True,
                val=True,  # ê²€ì¦ í™œì„±í™”
                plots=True,  # í”Œë¡¯ ìƒì„±
                save_period=10  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©
            )
            
            # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            if hasattr(results, 'save_dir') and results.save_dir:
                best_model_path = results.save_dir / 'weights' / 'best.pt'
                model_path = str(best_model_path)
            else:
                # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
                model_path = 'custom_training/custom_model/weights/best.pt'
            
            print(f"âœ… Enhanced training completed! Model saved at: {model_path}")
            
            return {
                'success': True,
                'model_path': model_path,
                'results': results
            }
            
        except Exception as e:
            print(f"âŒ Enhanced training error: {e}")
            return {'success': False, 'error': str(e)}
    
    def load_custom_model(self, model_path):
        """ì»¤ìŠ¤í…€ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            self.yolo_model = YOLO(model_path)
            global current_custom_model_path
            current_custom_model_path = model_path
            print(f"Custom model loaded from: {model_path}")
            return True
        except Exception as e:
            print(f"Custom model loading error: {e}")
            return False
    
    def generate_colors(self, num_classes):
        """í´ë˜ìŠ¤ë³„ ê³ ìœ í•œ ìƒ‰ìƒ ìƒì„±"""
        colors = []
        for i in range(num_classes):
            hue = i / num_classes
            saturation = 0.8
            value = 0.9
            rgb = colorsys.hsv_to_rgb(hue, saturation, value)
            # BGR í˜•íƒœë¡œ ë³€í™˜ (OpenCV ì‚¬ìš©)
            bgr = (int(rgb[2] * 255), int(rgb[1] * 255), int(rgb[0] * 255))
            colors.append(bgr)
        return colors
    
    def create_annotated_video(self, video_path, annotations, output_path):
        """ë°”ìš´ë”© ë°•ìŠ¤ê°€ ê·¸ë ¤ì§„ ë¹„ë””ì˜¤ ìƒì„±"""
        try:
            cap = cv2.VideoCapture(video_path)
            
            # ë¹„ë””ì˜¤ ì†ì„± ê°€ì ¸ì˜¤ê¸°
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
            fourcc = cv2.VideoWriter.fourcc(*'mp4v')  # type: ignore
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ìƒì„±
            unique_labels = list(set([ann['label'] for ann in annotations]))
            colors = self.generate_colors(len(unique_labels))
            label_colors = {label: colors[i] for i, label in enumerate(unique_labels)}
            
            frame_count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                current_time = frame_count / fps
                
                # í˜„ì¬ í”„ë ˆì„ì— í•´ë‹¹í•˜ëŠ” ì–´ë…¸í…Œì´ì…˜ ì°¾ê¸°
                current_annotations = [
                    ann for ann in annotations 
                    if abs(float(ann['frame']) - current_time) < (1.0 / fps)
                ]
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                for ann in current_annotations:
                    x, y, w, h = ann['bbox']
                    label = ann['label']
                    confidence = ann.get('confidence', 1.0)
                    source = ann.get('source', 'manual')
                    
                    # ìƒ‰ìƒ ì„ íƒ
                    color = label_colors.get(label, (0, 255, 0))
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
                    
                    # ë¼ë²¨ í…ìŠ¤íŠ¸ ì¤€ë¹„
                    if source == 'auto':
                        text = f"{label} ({confidence:.2f})"
                    else:
                        text = f"{label} (Manual)"
                    
                    # í…ìŠ¤íŠ¸ ë°°ê²½ í¬ê¸° ê³„ì‚°
                    (text_width, text_height), baseline = cv2.getTextSize(
                        text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
                    )
                    
                    # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
                    cv2.rectangle(
                        frame, 
                        (x, y - text_height - 10), 
                        (x + text_width, y), 
                        color, 
                        -1
                    )
                    
                    # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
                    cv2.putText(
                        frame, 
                        text, 
                        (x, y - 5), 
                        cv2.FONT_HERSHEY_SIMPLEX, 
                        0.6, 
                        (255, 255, 255), 
                        2
                    )
                
                # í”„ë ˆì„ ì €ì¥
                out.write(frame)
                frame_count += 1
            
            cap.release()
            out.release()
            
            return True
            
        except Exception as e:
            print(f"Video annotation error: {e}")
            return False

# ì „ì—­ ìë™ ë¼ë²¨ë§ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
auto_labeling = AutoLabelingSystem()

# ì‹œì‘ ì‹œ ì¦‰ì‹œ ëª¨ë¸ ë¡œë“œ ì‹œë„
print("ğŸš€ Initializing models at startup...")
model_load_success = auto_labeling.load_models()
if model_load_success:
    print("âœ… Models loaded successfully at startup!")
else:
    print("âš ï¸  Some models failed to load at startup. You can try manual initialization via /api/init_models")

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€ ì„œë¹™"""
    return send_from_directory('.', 'index.html')

@app.route('/<path:filename>')
def static_files(filename):
    """ì •ì  íŒŒì¼ ì„œë¹™"""
    return send_from_directory('.', filename)

@app.route('/api/init_models', methods=['POST'])
def init_models():
    """ëª¨ë¸ ì´ˆê¸°í™”"""
    try:
        success = auto_labeling.load_models()
        if success:
            return jsonify({"status": "success", "message": "Models loaded successfully"})
        else:
            return jsonify({"status": "error", "message": "Failed to load models"})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)})

@app.route('/api/upload_video', methods=['POST'])
def upload_video():
    """ë¹„ë””ì˜¤ ì—…ë¡œë“œ"""
    try:
        if 'video' not in request.files:
            return jsonify({"error": "No video file provided"}), 400
        
        file = request.files['video']
        if file.filename == '':
            return jsonify({"error": "No file selected"}), 400
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        filename = file.filename or 'uploaded_video.mp4'
        temp_path = os.path.join(UPLOAD_FOLDER, filename)
        file.save(temp_path)
        
        return jsonify({
            "status": "success", 
            "message": "Video uploaded successfully",
            "video_path": temp_path
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/auto_label', methods=['POST'])
def auto_label_video():
    """ìë™ ë¼ë²¨ë§ ì‹¤í–‰"""
    try:
        data = request.get_json()
        video_path = data.get('video_path')
        confidence_threshold = data.get('confidence_threshold', 0.2)
        dense_analysis = data.get('dense_analysis', True)
        
        if not video_path or not os.path.exists(video_path):
            return jsonify({"error": "Video file not found"}), 400
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì • (YOLO ëª¨ë¸ì— ì ìš©)
        if auto_labeling.yolo_model and hasattr(auto_labeling.yolo_model, 'conf'):
            auto_labeling.yolo_model.conf = confidence_threshold
        
        # ìë™ ë¼ë²¨ë§ ìˆ˜í–‰
        annotations = auto_labeling.process_video(video_path, dense_analysis)
        
        return jsonify({
            "status": "success",
            "annotations": annotations,
            "total_detections": len(annotations),
            "confidence_threshold": confidence_threshold,
            "dense_analysis": dense_analysis
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/status', methods=['GET'])
def get_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    global current_custom_model_path, training_data_accumulator
    
    return jsonify({
        "yolo_loaded": auto_labeling.yolo_model is not None,
        "dinov2_loaded": auto_labeling.dinov2_model is not None,
        "device": str(auto_labeling.device),
        "custom_model_path": current_custom_model_path,
        "accumulated_training_data": len(training_data_accumulator),
        "is_custom_model": current_custom_model_path is not None
    })

@app.route('/api/create_annotated_video', methods=['POST'])
def create_annotated_video():
    """ë°”ìš´ë”© ë°•ìŠ¤ê°€ ê·¸ë ¤ì§„ ë¹„ë””ì˜¤ ìƒì„±"""
    try:
        data = request.get_json()
        video_path = data.get('video_path')
        annotations = data.get('annotations', [])
        
        if not video_path or not os.path.exists(video_path):
            return jsonify({"error": "Video file not found"}), 400
        
        if not annotations:
            return jsonify({"error": "No annotations provided"}), 400
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„±
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        output_filename = f"{video_name}_annotated.mp4"
        output_path = os.path.join(UPLOAD_FOLDER, output_filename)
        
        # ì–´ë…¸í…Œì´ì…˜ëœ ë¹„ë””ì˜¤ ìƒì„±
        success = auto_labeling.create_annotated_video(video_path, annotations, output_path)
        
        if success:
            return jsonify({
                "status": "success",
                "message": "Annotated video created successfully",
                "output_path": output_path,
                "download_url": f"/api/download/{output_filename}"
            })
        else:
            return jsonify({"error": "Failed to create annotated video"}), 500
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/download/<filename>')
def download_file(filename):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        return send_from_directory(UPLOAD_FOLDER, filename, as_attachment=True)
    except Exception as e:
        return jsonify({"error": str(e)}), 404

@app.route('/api/train_custom_model', methods=['POST'])
def train_custom_model():
    """ì»¤ìŠ¤í…€ ëª¨ë¸ í•™ìŠµ"""
    try:
        data = request.get_json()
        video_path = data.get('video_path')
        annotations = data.get('annotations', [])
        epochs = data.get('epochs', 30)
        
        if not video_path or not os.path.exists(video_path):
            return jsonify({"error": "Video file not found"}), 400
        
        # ìˆ˜ë™ ë¼ë²¨ë§ ë°ì´í„°ë§Œ í•„í„°ë§ (sourceê°€ 'manual'ì´ê±°ë‚˜ ì—†ëŠ” ê²½ìš°ë„ í¬í•¨)
        manual_annotations = [ann for ann in annotations if ann.get('source') == 'manual' or ann.get('source') is None]
        
        if not manual_annotations:
            return jsonify({"error": "No manual annotations found for training"}), 400
        
        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ëˆ„ì  ë°ì´í„° í¬í•¨)
        training_dir = os.path.join(UPLOAD_FOLDER, 'training_data')
        global training_data_accumulator
        prep_result = auto_labeling.prepare_training_data(video_path, annotations, training_dir, training_data_accumulator)
        
        if not prep_result['success']:
            return jsonify({"error": prep_result['error']}), 500
        
        # ëª¨ë¸ í•™ìŠµ
        train_result = auto_labeling.train_custom_model(prep_result['dataset_path'], epochs)
        
        if train_result['success']:
            # í˜„ì¬ ë¼ë²¨ë§ ë°ì´í„°ë¥¼ ëˆ„ì  ë°ì´í„°ì— ì¶”ê°€
            training_data_accumulator.extend(prep_result['current_annotations'])
            
            # ì»¤ìŠ¤í…€ ëª¨ë¸ ìë™ ë¡œë“œ
            auto_labeling.load_custom_model(train_result['model_path'])
            
            return jsonify({
                "status": "success",
                "message": "Custom model training completed",
                "model_path": train_result['model_path'],
                "images_count": prep_result['images_count'],
                "total_annotations": prep_result['total_annotations'],
                "classes": prep_result['classes']
            })
        else:
            return jsonify({"error": train_result['error']}), 500
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/load_custom_model', methods=['POST'])
def load_custom_model():
    """í•™ìŠµëœ ì»¤ìŠ¤í…€ ëª¨ë¸ ë¡œë“œ"""
    try:
        data = request.get_json()
        model_path = data.get('model_path')
        
        if not model_path or not os.path.exists(model_path):
            return jsonify({"error": "Model file not found"}), 400
        
        success = auto_labeling.load_custom_model(model_path)
        
        if success:
            return jsonify({
                "status": "success",
                "message": "Custom model loaded successfully"
            })
        else:
            return jsonify({"error": "Failed to load custom model"}), 500
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/create_custom_model_video', methods=['POST'])
def create_custom_model_video():
    """í•™ìŠµëœ ì»¤ìŠ¤í…€ ëª¨ë¸ë¡œ ë°”ìš´ë”©ë°•ìŠ¤ ë¹„ë””ì˜¤ ìƒì„±"""
    try:
        data = request.get_json()
        video_path = data.get('video_path')
        
        if not video_path or not os.path.exists(video_path):
            return jsonify({"error": "Video file not found"}), 400
        
        if not auto_labeling.yolo_model:
            return jsonify({"error": "No model loaded"}), 400
        
        # ì¡°ë°€í•œ ë¶„ì„ ì˜µì…˜ í™•ì¸
        dense_analysis = data.get('dense_analysis', True)
        
        # ì»¤ìŠ¤í…€ ëª¨ë¸ë¡œ ì „ì²´ ë¹„ë””ì˜¤ ìë™ ë¼ë²¨ë§
        print(f"Creating annotated video with custom model (dense_analysis: {dense_analysis})...")
        annotations = auto_labeling.process_video(video_path, dense_analysis)
        
        if not annotations:
            return jsonify({"error": "No objects detected in video"}), 400
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„±
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        output_filename = f"{video_name}_custom_model.mp4"
        output_path = os.path.join(UPLOAD_FOLDER, output_filename)
        
        # ì–´ë…¸í…Œì´ì…˜ëœ ë¹„ë””ì˜¤ ìƒì„±
        success = auto_labeling.create_annotated_video(video_path, annotations, output_path)
        
        if success:
            return jsonify({
                "status": "success",
                "message": "Custom model video created successfully",
                "output_path": output_path,
                "download_url": f"/api/download/{output_filename}",
                "annotations": annotations,
                "total_detections": len(annotations)
            })
        else:
            return jsonify({"error": "Failed to create annotated video"}), 500
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/learn_dinov2_patterns', methods=['POST'])
def learn_dinov2_patterns():
    """ìˆ˜ë™ ë¼ë²¨ë§ ë°ì´í„°ì—ì„œ DINOv2 íŒ¨í„´ í•™ìŠµ"""
    try:
        data = request.get_json()
        video_path = data.get('video_path')
        annotations = data.get('annotations', [])
        
        if not video_path or not os.path.exists(video_path):
            return jsonify({"error": "Video file not found"}), 400
        
        # ìˆ˜ë™ ë¼ë²¨ë§ ë°ì´í„°ë§Œ í•„í„°ë§
        manual_annotations = [ann for ann in annotations if ann.get('source') == 'manual' or ann.get('source') is None]
        
        if not manual_annotations:
            return jsonify({"error": "No manual annotations found for learning"}), 400
        
        # DINOv2 ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not auto_labeling.dinov2_model:
            return jsonify({"error": "DINOv2 model not loaded"}), 400
        
        # íŒ¨í„´ í•™ìŠµ ì‹¤í–‰
        success = auto_labeling.universal_classifier.learn_from_manual_annotations(
            video_path, manual_annotations, auto_labeling
        )
        
        if success:
            # í•™ìŠµëœ íŒ¨í„´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            learned_info = auto_labeling.universal_classifier.get_learned_labels_info()
            
            return jsonify({
                "status": "success",
                "message": "DINOv2 patterns learned successfully",
                "learned_labels": learned_info,
                "manual_annotations_count": len(manual_annotations)
            })
        else:
            return jsonify({"error": "Failed to learn DINOv2 patterns"}), 500
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/get_dinov2_patterns_info', methods=['GET'])
def get_dinov2_patterns_info():
    """í•™ìŠµëœ DINOv2 íŒ¨í„´ ì •ë³´ ì¡°íšŒ"""
    try:
        learned_info = auto_labeling.universal_classifier.get_learned_labels_info()
        
        return jsonify({
            "status": "success",
            "learned_labels": learned_info,
            "total_labels": len(learned_info),
            "total_samples": sum(info['sample_count'] for info in learned_info.values()),
            "has_clustering": any(info['has_clustering'] for info in learned_info.values())
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/memory_status', methods=['GET'])
def get_memory_status():
    """ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
    try:
        current_usage = auto_labeling.get_current_memory_usage()
        memory_limit = auto_labeling.memory_limit
        
        return jsonify({
            "memory_usage": current_usage,
            "memory_limit": memory_limit,
            "percentage": (current_usage / memory_limit * 100),
            "dinov2_active": auto_labeling.dinov2_model is not None
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ¬ Starting Video Labeling Backend Server...")
    print("=" * 60)
    
    # ëª¨ë¸ì€ ì´ë¯¸ ì‹œì‘ ì‹œ ë¡œë“œë¨
    if auto_labeling.yolo_model:
        print("ğŸ¯ Ready to process videos!")
    else:
        print("âš ï¸  YOLO model not loaded. Please check the logs above.")
    
    print("ğŸŒ Server starting on http://localhost:5000")
    print("=" * 60)
    app.run(debug=True, host='0.0.0.0', port=5000) 