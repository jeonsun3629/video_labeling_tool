"""
YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸°
1ë‹¨ê³„: YOLOë¡œ ê°ì²´ íƒì§€ (ìœ„ì¹˜ ì •ë³´)
2ë‹¨ê³„: CLIPìœ¼ë¡œ ê° ê°ì²´ì˜ ë¶ˆëŸ‰ ì—¬ë¶€ ë¶„ë¥˜
"""
import torch
import cv2
import numpy as np
from typing import List, Dict, Any, Optional
from PIL import Image
from ..base.base_detector import BaseDetector
from .yolo_detector import YOLODetector
from config.settings import YOLO_MODEL_PATH, CLIP_DEFECT_THRESHOLD

class YOLOCLIPHybridDetector(BaseDetector):
    """YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸°"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.yolo_detector = None
        self.clip_model = None
        self.clip_processor = None
        self.defect_queries = kwargs.get('defect_queries', [
            "a photo of a defective product",
            "a photo of a damaged item", 
            "a photo of a broken object",
            "a photo of a quality defect",
            "a photo of a normal product",
            "a photo of a good quality item"
        ])
        self.defect_threshold = kwargs.get('defect_threshold', CLIP_DEFECT_THRESHOLD)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def load_model(self) -> bool:
        """YOLOì™€ CLIP ëª¨ë¸ ëª¨ë‘ ë¡œë“œ"""
        try:
            print("ğŸ“¥ Loading YOLO + CLIP hybrid detector...")
            
            # 1. YOLO ëª¨ë¸ ë¡œë“œ
            self.yolo_detector = YOLODetector()
            yolo_success = self.yolo_detector.load_model()
            
            if not yolo_success:
                print("âŒ Failed to load YOLO model")
                return False
            
            # 2. CLIP ëª¨ë¸ ë¡œë“œ
            try:
                import clip
                self.clip_model, self.clip_processor = clip.load("ViT-B/32", device=self.device)
                print("âœ… YOLO + CLIP hybrid detector loaded successfully!")
                return True
                
            except ImportError:
                print("âŒ CLIP not installed. Install with: pip install git+https://github.com/openai/CLIP.git")
                return False
                
        except Exception as e:
            print(f"âŒ Failed to load hybrid detector: {e}")
            return False
    
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€"""
        if not self.is_loaded():
            print("ERROR: Hybrid detector not loaded!")
            return []
        
        try:
            # 1ë‹¨ê³„: YOLOë¡œ ê°ì²´ íƒì§€
            yolo_detections = self.yolo_detector.detect(frame)
            
            if not yolo_detections:
                print("No objects detected by YOLO")
                return []
            
            print(f"ğŸ¯ YOLO detected {len(yolo_detections)} objects")
            
            # 2ë‹¨ê³„: ê° íƒì§€ëœ ê°ì²´ë¥¼ CLIPìœ¼ë¡œ ë¶„ë¥˜
            enhanced_detections = []
            
            for i, detection in enumerate(yolo_detections):
                try:
                    # ê°ì²´ ì˜ì—­ í¬ë¡­
                    x, y, w, h = detection['bbox']
                    crop = frame[y:y+h, x:x+w]
                    
                    if crop.size == 0:
                        continue
                    
                    # CLIPìœ¼ë¡œ ë¶ˆëŸ‰ ì—¬ë¶€ ë¶„ì„
                    defect_info = self._analyze_defect_with_clip(crop)
                    
                    # ê¸°ì¡´ íƒì§€ ì •ë³´ì— CLIP ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    enhanced_detection = detection.copy()
                    enhanced_detection.update({
                        'is_defective': defect_info['is_defective'],
                        'defect_confidence': defect_info['confidence'],
                        'defect_type': defect_info['defect_type'],
                        'clip_analysis': defect_info['analysis'],
                        'method': 'yolo_clip_hybrid'
                    })
                    
                    # ë¶ˆëŸ‰í’ˆìœ¼ë¡œ íŒì •ëœ ê²½ìš°ë§Œ ê²°ê³¼ì— í¬í•¨ (ê¸°ì¡´ YOLO í´ë˜ìŠ¤ëŠ” ì œì™¸)
                    if defect_info['is_defective']:
                        enhanced_detection['class_name'] = f"{detection['class_name']}_defective"
                        enhanced_detection['original_class'] = detection['class_name']
                        enhanced_detection['label'] = f"{detection['class_name']}_defective"
                        print(f"ğŸš¨ Defective {detection['class_name']} detected with confidence {defect_info['confidence']:.3f}")
                        enhanced_detections.append(enhanced_detection)
                    else:
                        # ì •ìƒ ì œí’ˆì€ ê²°ê³¼ì—ì„œ ì œì™¸ (ê¸°ì¡´ YOLO í´ë˜ìŠ¤ í•„í„°ë§)
                        print(f"âœ… Normal {detection['class_name']} filtered out (confidence: {defect_info['confidence']:.3f})")
                    
                except Exception as e:
                    print(f"âš ï¸ Error analyzing detection {i}: {e}")
                    # ì—ëŸ¬ ì‹œ ì›ë³¸ íƒì§€ ê²°ê³¼ ìœ ì§€
                    enhanced_detections.append(detection)
            
            print(f"ğŸ§  CLIP analysis completed for {len(enhanced_detections)} objects")
            return enhanced_detections
            
        except Exception as e:
            print(f"âŒ Hybrid detection error: {e}")
            return []
    
    def _analyze_defect_with_clip(self, crop: np.ndarray) -> Dict[str, Any]:
        """CLIPì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡­ëœ ì´ë¯¸ì§€ì˜ ë¶ˆëŸ‰ ì—¬ë¶€ ë¶„ì„"""
        try:
            import clip
            
            # BGR to RGB ë³€í™˜
            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(crop_rgb)
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            image_input = self.clip_processor(pil_image).unsqueeze(0).to(self.device)
            
            # í…ìŠ¤íŠ¸ ì¿¼ë¦¬ í† í°í™”
            text_inputs = clip.tokenize(self.defect_queries).to(self.device)
            
            with torch.no_grad():
                # íŠ¹ì§• ì¶”ì¶œ
                image_features = self.clip_model.encode_image(image_input)
                text_features = self.clip_model.encode_text(text_inputs)
                
                # ìœ ì‚¬ë„ ê³„ì‚°
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)
                similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
                
                # ê° ì¿¼ë¦¬ë³„ ì ìˆ˜
                scores = similarity[0].cpu().numpy()
                
                # defect ê´€ë ¨ ì¿¼ë¦¬ë“¤ (ì²˜ìŒ 4ê°œ)ì™€ normal ê´€ë ¨ ì¿¼ë¦¬ë“¤ (ë‚˜ë¨¸ì§€ 2ê°œ) ë¶„ë¦¬
                defect_scores = scores[:4]  # defective, damaged, broken, quality defect
                normal_scores = scores[4:]  # normal, good quality
                
                max_defect_score = defect_scores.max()
                max_normal_score = normal_scores.max()
                
                # ë¶ˆëŸ‰ ì—¬ë¶€ íŒì •
                is_defective = max_defect_score > max_normal_score and max_defect_score > self.defect_threshold
                
                # ìµœê³  ì ìˆ˜ ì¿¼ë¦¬ ì°¾ê¸°
                if is_defective:
                    best_defect_idx = defect_scores.argmax()
                    defect_type = self.defect_queries[best_defect_idx]
                    confidence = float(max_defect_score)
                else:
                    best_normal_idx = normal_scores.argmax()
                    defect_type = self.defect_queries[4 + best_normal_idx]  # normal ì¿¼ë¦¬ ì¸ë±ìŠ¤
                    confidence = float(max_normal_score)
                
                return {
                    'is_defective': is_defective,
                    'confidence': confidence,
                    'defect_type': defect_type,
                    'analysis': {
                        'defect_scores': defect_scores.tolist(),
                        'normal_scores': normal_scores.tolist(),
                        'all_similarities': scores.tolist()
                    }
                }
                
        except Exception as e:
            print(f"âŒ CLIP analysis error: {e}")
            return {
                'is_defective': False,
                'confidence': 0.0,
                'defect_type': 'analysis_failed',
                'analysis': {}
            }
    
    def get_supported_classes(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” í´ë˜ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        if self.yolo_detector:
            base_classes = self.yolo_detector.get_supported_classes()
            # ê° í´ë˜ìŠ¤ì— ëŒ€í•´ defective ë²„ì „ë„ ì¶”ê°€
            extended_classes = base_classes.copy()
            for cls in base_classes:
                extended_classes.append(f"{cls}_defective")
            return extended_classes
        return ["defect", "normal"]
    
    def set_confidence_threshold(self, threshold: float) -> None:
        """ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •"""
        if self.yolo_detector:
            self.yolo_detector.set_confidence_threshold(threshold)
    
    def set_defect_threshold(self, threshold: float) -> None:
        """CLIP defect ì„ê³„ê°’ ì„¤ì •"""
        self.defect_threshold = threshold
    
    def is_loaded(self) -> bool:
        """í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸° ë¡œë“œ ìƒíƒœ í™•ì¸"""
        yolo_loaded = self.yolo_detector is not None and self.yolo_detector.is_loaded()
        clip_loaded = self.clip_model is not None
        return yolo_loaded and clip_loaded
    
    def get_model_info(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        base_info = super().get_model_info()
        base_info.update({
            'hybrid_type': 'yolo_clip',
            'yolo_loaded': self.yolo_detector is not None and self.yolo_detector.is_loaded(),
            'clip_loaded': self.clip_model is not None,
            'defect_threshold': self.defect_threshold,
            'defect_queries': self.defect_queries
        })
        return base_info
