"""
YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸°
1ë‹¨ê³„: YOLOë¡œ ê°ì²´ íƒì§€ (ìœ„ì¹˜ ì •ë³´)
2ë‹¨ê³„: CLIPìœ¼ë¡œ ê° ê°ì²´ì˜ ë¶ˆëŸ‰ ì—¬ë¶€ ë¶„ë¥˜
"""
import torch
import cv2
import numpy as np
from typing import List, Dict, Any, Optional
from PIL import Image
from ..base.base_detector import BaseDetector
from .yolo_detector import YOLODetector
from config.settings import YOLO_MODEL_PATH, CLIP_DEFECT_THRESHOLD

class YOLOCLIPHybridDetector(BaseDetector):
    """YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸°"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.yolo_detector = None
        self.clip_model = None
        self.clip_processor = None
        self.defect_queries = kwargs.get('defect_queries', [
            "a photo of a defective product",
            "a photo of a damaged item", 
            "a photo of a broken object",
            "a photo of a quality defect",
            "a photo of a normal product",
            "a photo of a good quality item"
        ])
        print(f"ğŸ”§ YOLO+CLIP initialized with {len(self.defect_queries)} defect queries")
        self.defect_threshold = kwargs.get('defect_threshold', CLIP_DEFECT_THRESHOLD)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def load_model(self) -> bool:
        """YOLOì™€ CLIP ëª¨ë¸ ëª¨ë‘ ë¡œë“œ"""
        try:
            print("ğŸ“¥ Loading YOLO + CLIP hybrid detector...")
            
            # 1. YOLO ëª¨ë¸ ë¡œë“œ
            self.yolo_detector = YOLODetector()
            yolo_success = self.yolo_detector.load_model()
            
            if not yolo_success:
                print("âŒ Failed to load YOLO model")
                return False
            
            # 2. CLIP ëª¨ë¸ ë¡œë“œ
            try:
                import clip
                print("ğŸ”„ Loading CLIP ViT-B/32 model...")
                self.clip_model, self.clip_processor = clip.load("ViT-B/32", device=self.device)
                self.clip_model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
                print("âœ… YOLO + CLIP hybrid detector loaded successfully!")
                print(f"   YOLO classes: {len(self.yolo_detector.get_supported_classes())}")
                print(f"   CLIP device: {self.device}")
                print(f"   Defect threshold: {self.defect_threshold}")
                return True
                
            except ImportError as e:
                print(f"âŒ CLIP import error: {e}")
                print("âŒ Install CLIP with: pip install git+https://github.com/openai/CLIP.git")
                return False
            except Exception as e:
                print(f"âŒ CLIP loading error: {e}")
                return False
                
        except Exception as e:
            print(f"âŒ Failed to load hybrid detector: {e}")
            return False
    
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€"""
        if not self.is_loaded():
            print("âŒ ERROR: Hybrid detector not loaded!")
            return []
        
        try:
            # 1ë‹¨ê³„: YOLOë¡œ ê°ì²´ íƒì§€
            print("ğŸ” Running YOLO detection...")
            yolo_detections = self.yolo_detector.detect(frame)
            
            if not yolo_detections:
                print("â„¹ï¸ No objects detected by YOLO")
                return []
            
            print(f"ğŸ¯ YOLO detected {len(yolo_detections)} objects")
            
            # 2ë‹¨ê³„: ê° íƒì§€ëœ ê°ì²´ë¥¼ CLIPìœ¼ë¡œ ë¶„ë¥˜
            enhanced_detections = []
            successful_analyses = 0
            
            for i, detection in enumerate(yolo_detections):
                try:
                    # ê°ì²´ ì˜ì—­ í¬ë¡­
                    x, y, w, h = detection['bbox']
                    
                    # ê²½ê³„ í™•ì¸
                    if x < 0 or y < 0 or x + w > frame.shape[1] or y + h > frame.shape[0]:
                        print(f"âš ï¸ Invalid bbox for detection {i}: {detection['bbox']}")
                        continue
                    
                    crop = frame[y:y+h, x:x+w]
                    
                    if crop.size == 0:
                        print(f"âš ï¸ Empty crop for detection {i}")
                        continue
                    
                    # CLIPìœ¼ë¡œ ë¶ˆëŸ‰ ì—¬ë¶€ ë¶„ì„
                    print(f"ğŸ” Analyzing object {i+1}/{len(yolo_detections)} with CLIP...")
                    defect_info = self._analyze_defect_with_clip(crop)
                    
                    if defect_info['defect_type'] == 'analysis_failed':
                        print(f"âš ï¸ CLIP analysis failed for detection {i}")
                        continue
                    
                    successful_analyses += 1
                    
                    # ê¸°ì¡´ íƒì§€ ì •ë³´ì— CLIP ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    enhanced_detection = detection.copy()
                    enhanced_detection.update({
                        'is_defective': defect_info['is_defective'],
                        'defect_confidence': defect_info['confidence'],
                        'defect_type': defect_info['defect_type'],
                        'clip_analysis': defect_info['analysis'],
                        'method': 'yolo_clip_hybrid'
                    })
                    
                    # ë¶ˆëŸ‰í’ˆìœ¼ë¡œ íŒì •ëœ ê²½ìš°ë§Œ ê²°ê³¼ì— í¬í•¨ (ê¸°ì¡´ YOLO í´ë˜ìŠ¤ëŠ” ì œì™¸)
                    if defect_info['is_defective']:
                        enhanced_detection['class_name'] = f"{detection['class_name']}_defective"
                        enhanced_detection['original_class'] = detection['class_name']
                        enhanced_detection['label'] = f"{detection['class_name']}_defective"
                        print(f"ğŸš¨ Defective {detection['class_name']} detected with confidence {defect_info['confidence']:.3f}")
                        enhanced_detections.append(enhanced_detection)
                    else:
                        # ì •ìƒ ì œí’ˆì€ ê²°ê³¼ì—ì„œ ì œì™¸ (ê¸°ì¡´ YOLO í´ë˜ìŠ¤ í•„í„°ë§)
                        print(f"âœ… Normal {detection['class_name']} filtered out (confidence: {defect_info['confidence']:.3f})")
                    
                except Exception as e:
                    print(f"âš ï¸ Error analyzing detection {i}: {e}")
                    # ì—ëŸ¬ ì‹œ ì›ë³¸ íƒì§€ ê²°ê³¼ ìœ ì§€í•˜ì§€ ì•ŠìŒ (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì˜ ëª©ì ì— ë§ì§€ ì•ŠìŒ)
                    continue
            
            print(f"ğŸ§  CLIP analysis completed: {successful_analyses}/{len(yolo_detections)} successful")
            print(f"ğŸ¯ Final results: {len(enhanced_detections)} defective objects detected")
            return enhanced_detections
            
        except Exception as e:
            print(f"âŒ Hybrid detection error: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _analyze_defect_with_clip(self, crop: np.ndarray) -> Dict[str, Any]:
        """CLIPì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡­ëœ ì´ë¯¸ì§€ì˜ ë¶ˆëŸ‰ ì—¬ë¶€ ë¶„ì„"""
        try:
            import clip
            
            # ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬
            if crop is None or crop.size == 0:
                print("âš ï¸ Invalid crop image for CLIP analysis")
                return self._get_default_analysis_result()
            
            # í¬ë¡­ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì€ ê²½ìš° ì²˜ë¦¬
            if crop.shape[0] < 32 or crop.shape[1] < 32:
                print(f"âš ï¸ Crop too small for reliable analysis: {crop.shape}")
                return self._get_default_analysis_result()
            
            # BGR to RGB ë³€í™˜
            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(crop_rgb)
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            image_input = self.clip_processor(pil_image).unsqueeze(0).to(self.device)
            
            # í…ìŠ¤íŠ¸ ì¿¼ë¦¬ í† í°í™” (ì•ˆì „í•œ ì²˜ë¦¬)
            try:
                text_inputs = clip.tokenize(self.defect_queries).to(self.device)
                if text_inputs.size(0) != len(self.defect_queries):
                    print(f"âš ï¸ Text tokenization mismatch: {text_inputs.size(0)} vs {len(self.defect_queries)}")
                    return self._get_default_analysis_result()
            except Exception as e:
                print(f"âš ï¸ Text tokenization error: {e}")
                return self._get_default_analysis_result()
            
            with torch.no_grad():
                # íŠ¹ì§• ì¶”ì¶œ
                image_features = self.clip_model.encode_image(image_input)
                text_features = self.clip_model.encode_text(text_inputs)
                
                # ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”)
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                logits = (100.0 * image_features @ text_features.T)
                probs = logits.softmax(dim=-1)
                
                # ê° ì¿¼ë¦¬ë³„ ì ìˆ˜
                scores = probs[0].cpu().numpy()
                print(f"ğŸ” CLIP scores shape: {scores.shape}, expected: {len(self.defect_queries)}")
                print(f"ğŸ” CLIP scores: {scores}")
                
                # ì ìˆ˜ ë°°ì—´ ìœ íš¨ì„± ê²€ì‚¬
                if len(scores) < len(self.defect_queries):
                    print(f"âš ï¸ Insufficient scores: {len(scores)} < {len(self.defect_queries)}")
                    return self._get_default_analysis_result()
                
                # defect ê´€ë ¨ ì¿¼ë¦¬ë“¤ (ì²˜ìŒ 4ê°œ)ì™€ normal ê´€ë ¨ ì¿¼ë¦¬ë“¤ (ë‚˜ë¨¸ì§€ 2ê°œ) ë¶„ë¦¬
                defect_scores = scores[:4]  # defective, damaged, broken, quality defect
                normal_scores = scores[4:]  # normal, good quality
                
                # ë¹ˆ ë°°ì—´ ê²€ì‚¬
                if len(defect_scores) == 0 or len(normal_scores) == 0:
                    print(f"âš ï¸ Empty score arrays: defect={len(defect_scores)}, normal={len(normal_scores)}")
                    return self._get_default_analysis_result()
                
                max_defect_score = defect_scores.max() if len(defect_scores) > 0 else 0.0
                max_normal_score = normal_scores.max() if len(normal_scores) > 0 else 0.0
                
                # ë¶ˆëŸ‰ ì—¬ë¶€ íŒì • (ë” ì—„ê²©í•œ ê¸°ì¤€)
                defect_sum = defect_scores.sum() if len(defect_scores) > 0 else 0.0
                normal_sum = normal_scores.sum() if len(normal_scores) > 0 else 0.0
                
                # ìµœê³  ì ìˆ˜ì™€ ì´í•© ì ìˆ˜ë¥¼ ëª¨ë‘ ê³ ë ¤
                is_defective = (max_defect_score > max_normal_score and 
                              defect_sum > normal_sum and 
                              max_defect_score > self.defect_threshold)
                
                # ìµœê³  ì ìˆ˜ ì¿¼ë¦¬ ì°¾ê¸°
                if is_defective and len(defect_scores) > 0:
                    best_defect_idx = defect_scores.argmax()
                    defect_type = self.defect_queries[best_defect_idx]
                    confidence = float(max_defect_score)
                elif len(normal_scores) > 0:
                    best_normal_idx = normal_scores.argmax()
                    defect_type = self.defect_queries[4 + best_normal_idx]  # normal ì¿¼ë¦¬ ì¸ë±ìŠ¤
                    confidence = float(max_normal_score)
                else:
                    # ëª¨ë“  ì ìˆ˜ê°€ ì—†ëŠ” ê²½ìš°
                    defect_type = "unknown"
                    confidence = 0.0
                
                return {
                    'is_defective': is_defective,
                    'confidence': confidence,
                    'defect_type': defect_type,
                    'analysis': {
                        'defect_scores': defect_scores.tolist(),
                        'normal_scores': normal_scores.tolist(),
                        'all_similarities': scores.tolist(),
                        'defect_sum': float(defect_sum),
                        'normal_sum': float(normal_sum)
                    }
                }
                
        except Exception as e:
            print(f"âŒ CLIP analysis error: {e}")
            return self._get_default_analysis_result()
    
    def _get_default_analysis_result(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜ (ì—ëŸ¬ ì‹œ ì‚¬ìš©)"""
        return {
            'is_defective': False,
            'confidence': 0.0,
            'defect_type': 'analysis_failed',
            'analysis': {}
        }
    
    def get_supported_classes(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” í´ë˜ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        if self.yolo_detector:
            base_classes = self.yolo_detector.get_supported_classes()
            # ê° í´ë˜ìŠ¤ì— ëŒ€í•´ defective ë²„ì „ë„ ì¶”ê°€
            extended_classes = base_classes.copy()
            for cls in base_classes:
                extended_classes.append(f"{cls}_defective")
            return extended_classes
        return ["defect", "normal"]
    
    def set_confidence_threshold(self, threshold: float) -> None:
        """ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •"""
        if self.yolo_detector:
            self.yolo_detector.set_confidence_threshold(threshold)
    
    def set_defect_threshold(self, threshold: float) -> None:
        """CLIP defect ì„ê³„ê°’ ì„¤ì •"""
        self.defect_threshold = threshold
    
    def is_loaded(self) -> bool:
        """í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸° ë¡œë“œ ìƒíƒœ í™•ì¸"""
        yolo_loaded = self.yolo_detector is not None and self.yolo_detector.is_loaded()
        clip_loaded = self.clip_model is not None
        return yolo_loaded and clip_loaded
    
    def get_model_info(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        base_info = super().get_model_info()
        base_info.update({
            'hybrid_type': 'yolo_clip',
            'yolo_loaded': self.yolo_detector is not None and self.yolo_detector.is_loaded(),
            'clip_loaded': self.clip_model is not None,
            'defect_threshold': self.defect_threshold,
            'defect_queries': self.defect_queries
        })
        return base_info
