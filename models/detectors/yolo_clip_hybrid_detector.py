"""
YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸°
1ë‹¨ê³„: YOLOë¡œ ê°ì²´ íƒì§€ (ìœ„ì¹˜ ì •ë³´)
2ë‹¨ê³„: CLIPìœ¼ë¡œ ê° ê°ì²´ì˜ ë¶ˆëŸ‰ ì—¬ë¶€ ë¶„ë¥˜
"""
import torch
import cv2
import numpy as np
from typing import List, Dict, Any, Optional
from PIL import Image
from ..base.base_detector import BaseDetector
from .yolo_detector import YOLODetector
from config.settings import YOLO_MODEL_PATH, CLIP_DEFECT_THRESHOLD

class YOLOCLIPHybridDetector(BaseDetector):
    """YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸°"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.yolo_detector = None
        self.clip_model = None
        self.clip_processor = None
        # YOLO ëª¨ë¸ ê´€ë ¨ ì„¤ì • ì €ì¥
        self.yolo_kwargs = {k: v for k, v in kwargs.items() if k in ['model_path', 'confidence_threshold']}
        # ì‚¬ìš©ì ì •ì˜ CLIP ì¿¼ë¦¬ (UIì—ì„œ ì„¤ì • ê°€ëŠ¥)
        self.custom_queries = kwargs.get('defect_queries', [
            # ê¸°ë³¸ ì˜ˆì‹œ ì¿¼ë¦¬ë“¤
            "a photo of a person under the car",
            "a person working under a vehicle", 
            "a mechanic under the car",
            "a person lying under the car",
            # ëŒ€ì¡°êµ°
            "a person standing next to the car",
            "a normal person near the car"
        ])
        
        # ì‚¬ìš©ì ì •ì˜ ë¼ë²¨ ë§¤í•‘ ìë™ ìƒì„±
        self.query_labels = kwargs.get('query_labels', self._generate_query_labels())
        
        # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
        self.defect_queries = self.custom_queries
        
        print(f"ğŸ”§ YOLO+CLIP initialized with {len(self.custom_queries)} custom queries")
        self.defect_threshold = kwargs.get('defect_threshold', CLIP_DEFECT_THRESHOLD)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def load_model(self) -> bool:
        """YOLOì™€ CLIP ëª¨ë¸ ëª¨ë‘ ë¡œë“œ"""
        try:
            print("ğŸ“¥ Loading YOLO + CLIP hybrid detector...")
            
            # 1. YOLO ëª¨ë¸ ë¡œë“œ (ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ í¬í•¨)
            self.yolo_detector = YOLODetector(**self.yolo_kwargs)
            yolo_success = self.yolo_detector.load_model()
            
            if not yolo_success:
                print("âŒ Failed to load YOLO model")
                return False
            
            # 2. CLIP ëª¨ë¸ ë¡œë“œ
            try:
                import clip
                print("ğŸ”„ Loading CLIP ViT-B/32 model...")
                self.clip_model, self.clip_processor = clip.load("ViT-B/32", device=self.device)
                self.clip_model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
                print("âœ… YOLO + CLIP hybrid detector loaded successfully!")
                print(f"   YOLO classes: {len(self.yolo_detector.get_supported_classes())}")
                print(f"   CLIP device: {self.device}")
                print(f"   Defect threshold: {self.defect_threshold}")
                return True
                
            except ImportError as e:
                print(f"âŒ CLIP import error: {e}")
                print("âŒ Install CLIP with: pip install git+https://github.com/openai/CLIP.git")
                return False
            except Exception as e:
                print(f"âŒ CLIP loading error: {e}")
                return False
                
        except Exception as e:
            print(f"âŒ Failed to load hybrid detector: {e}")
            return False
    
    def detect(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """YOLO + CLIP í•˜ì´ë¸Œë¦¬ë“œ íƒì§€"""
        if not self.is_loaded():
            print("âŒ ERROR: Hybrid detector not loaded!")
            return []
        
        try:
            # 1ë‹¨ê³„: YOLOë¡œ ê°ì²´ íƒì§€
            print("ğŸ” Running YOLO detection...")
            yolo_detections = self.yolo_detector.detect(frame)
            
            if not yolo_detections:
                print("â„¹ï¸ No objects detected by YOLO")
                return []
            
            print(f"ğŸ¯ YOLO detected {len(yolo_detections)} objects")
            
            # 2ë‹¨ê³„: ê° íƒì§€ëœ ê°ì²´ë¥¼ CLIPìœ¼ë¡œ ë¶„ë¥˜
            enhanced_detections = []
            successful_analyses = 0
            
            for i, detection in enumerate(yolo_detections):
                try:
                    # ê°ì²´ ì˜ì—­ í¬ë¡­
                    x, y, w, h = detection['bbox']
                    
                    # ê²½ê³„ í™•ì¸
                    if x < 0 or y < 0 or x + w > frame.shape[1] or y + h > frame.shape[0]:
                        print(f"âš ï¸ Invalid bbox for detection {i}: {detection['bbox']}")
                        continue
                    
                    crop = frame[y:y+h, x:x+w]
                    
                    if crop.size == 0:
                        print(f"âš ï¸ Empty crop for detection {i}")
                        continue
                    
                    # CLIPìœ¼ë¡œ ì‚¬ìš©ì ì •ì˜ ìƒí™© ë¶„ì„
                    print(f"ğŸ” Analyzing object {i+1}/{len(yolo_detections)} with CLIP...")
                    clip_info = self._analyze_with_clip(crop)
                    
                    if not clip_info or clip_info.get('error'):
                        print(f"âš ï¸ CLIP analysis failed for detection {i}")
                        continue
                    
                    successful_analyses += 1
                    
                    # ê¸°ì¡´ íƒì§€ ì •ë³´ì— CLIP ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    enhanced_detection = detection.copy()
                    enhanced_detection.update({
                        'clip_matched': clip_info['matched'],
                        'clip_confidence': clip_info['confidence'],
                        'clip_query': clip_info['matched_query'],
                        'clip_label': clip_info['matched_label'],
                        'clip_analysis': clip_info['analysis'],
                        'method': 'yolo_clip_hybrid'
                    })
                    
                    # CLIPì´ ì‚¬ìš©ì ì •ì˜ ìƒí™©ì„ ë§¤ì¹­í•œ ê²½ìš°
                    if clip_info['matched']:
                        enhanced_detection['class_name'] = clip_info['matched_label']
                        enhanced_detection['original_yolo_class'] = detection['class_name']
                        enhanced_detection['label'] = clip_info['matched_label']
                        print(f"ğŸ¯ CLIP matched: {detection['class_name']} â†’ {clip_info['matched_label']} (conf: {clip_info['confidence']:.3f})")
                        print(f"   Query: {clip_info['matched_query']}")
                        enhanced_detections.append(enhanced_detection)
                    else:
                        # ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ YOLO í´ë˜ìŠ¤ë¡œ ìœ ì§€í•˜ê±°ë‚˜ í•„í„°ë§
                        print(f"âŒ No CLIP match for {detection['class_name']} (best: {clip_info['confidence']:.3f})")
                        # ê¸°ë³¸ íƒì§€ë„ í¬í•¨í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                        # enhanced_detections.append(enhanced_detection)
                    
                except Exception as e:
                    print(f"âš ï¸ Error analyzing detection {i}: {e}")
                    # ì—ëŸ¬ ì‹œ ì›ë³¸ íƒì§€ ê²°ê³¼ ìœ ì§€í•˜ì§€ ì•ŠìŒ (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì˜ ëª©ì ì— ë§ì§€ ì•ŠìŒ)
                    continue
            
            print(f"ğŸ§  CLIP analysis completed: {successful_analyses}/{len(yolo_detections)} successful")
            print(f"ğŸ¯ Final results: {len(enhanced_detections)} defective objects detected")
            return enhanced_detections
            
        except Exception as e:
            print(f"âŒ Hybrid detection error: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _analyze_with_clip(self, crop: np.ndarray) -> Dict[str, Any]:
        """CLIPì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡­ëœ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©ì ì •ì˜ ì¿¼ë¦¬ì™€ ë§¤ì¹­"""
        try:
            import clip
            
            # ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬
            if crop is None or crop.size == 0:
                print("âš ï¸ Invalid crop image for CLIP analysis")
                return {'error': 'invalid_input'}
            
            # í¬ë¡­ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì€ ê²½ìš° ì²˜ë¦¬
            if crop.shape[0] < 32 or crop.shape[1] < 32:
                print(f"âš ï¸ Crop too small for reliable analysis: {crop.shape}")
                return {'error': 'crop_too_small'}
            
            # BGR to RGB ë³€í™˜
            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(crop_rgb)
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            image_input = self.clip_processor(pil_image).unsqueeze(0).to(self.device)
            
            # í…ìŠ¤íŠ¸ ì¿¼ë¦¬ í† í°í™”
            try:
                text_inputs = clip.tokenize(self.custom_queries).to(self.device)
                if text_inputs.size(0) != len(self.custom_queries):
                    print(f"âš ï¸ Text tokenization mismatch: {text_inputs.size(0)} vs {len(self.custom_queries)}")
                    return {'error': 'tokenization_mismatch'}
            except Exception as e:
                print(f"âš ï¸ Text tokenization error: {e}")
                return {'error': 'tokenization_failed'}
            
            with torch.no_grad():
                # íŠ¹ì§• ì¶”ì¶œ
                image_features = self.clip_model.encode_image(image_input)
                text_features = self.clip_model.encode_text(text_inputs)
                
                # ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”)
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                logits = (100.0 * image_features @ text_features.T)
                probs = logits.softmax(dim=-1)
                
                # ê° ì¿¼ë¦¬ë³„ ì ìˆ˜
                scores = probs[0].cpu().numpy()
                print(f"ğŸ” CLIP scores: {scores}")
                
                # ìµœê³  ì ìˆ˜ ì°¾ê¸°
                best_idx = scores.argmax()
                best_score = float(scores[best_idx])
                best_query = self.custom_queries[best_idx]
                best_label = self.query_labels[best_idx] if best_idx < len(self.query_labels) else "unknown"
                
                # ì„ê³„ê°’ í™•ì¸ (ì‚¬ìš©ì ì •ì˜ ìƒí™© ë§¤ì¹­ ì—¬ë¶€)
                matched = best_score > self.defect_threshold
                
                # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                print(f"ğŸ” CLIP Analysis Results:")
                print(f"   Best match: {best_query} (score: {best_score:.3f})")
                print(f"   Threshold: {self.defect_threshold}")
                print(f"   Matched: {'YES' if matched else 'NO'}")
                print(f"   Label: {best_label}")
                
                # ëª¨ë“  ì¿¼ë¦¬ ì ìˆ˜ ì¶œë ¥
                for i, (query, score) in enumerate(zip(self.custom_queries, scores)):
                    label = self.query_labels[i] if i < len(self.query_labels) else "unknown"
                    print(f"   {i+1}. [{label}] {query}: {score:.3f}")
                
                return {
                    'matched': matched,
                    'confidence': best_score,
                    'matched_query': best_query,
                    'matched_label': best_label,
                    'analysis': {
                        'all_scores': scores.tolist(),
                        'all_queries': self.custom_queries,
                        'all_labels': self.query_labels,
                        'best_index': int(best_idx),
                        'threshold': self.defect_threshold
                    }
                }
                
        except Exception as e:
            print(f"âŒ CLIP analysis error: {e}")
            return {'error': str(e)}
    
    def _generate_query_labels(self) -> List[str]:
        """ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ìë™ìœ¼ë¡œ ë¼ë²¨ ìƒì„±"""
        labels = []
        for query in self.custom_queries:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œë¡œ ë¼ë²¨ ìƒì„±
            query_lower = query.lower()
            
            if "under" in query_lower and ("car" in query_lower or "vehicle" in query_lower):
                labels.append("person_under_car")
            elif "mechanic" in query_lower:
                labels.append("mechanic_working")
            elif "normal" in query_lower or "standing" in query_lower:
                labels.append("normal_person")
            else:
                # ê¸°ë³¸ê°’: ì¿¼ë¦¬ì˜ í•µì‹¬ ë‹¨ì–´ë“¤ì„ ì¡°í•©
                words = [w for w in query_lower.replace("a photo of", "").replace("a person", "person").split() 
                        if len(w) > 2 and w not in ["the", "and", "with", "under", "near", "next"]]
                label = "_".join(words[:3]) if words else f"custom_{len(labels)}"
                labels.append(label)
        
        return labels
    
    def _get_default_analysis_result(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜ (ì—ëŸ¬ ì‹œ ì‚¬ìš©)"""
        return {
            'is_defective': False,
            'confidence': 0.0,
            'defect_type': 'analysis_failed',
            'analysis': {}
        }
    
    def get_supported_classes(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” í´ë˜ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        if self.yolo_detector:
            base_classes = self.yolo_detector.get_supported_classes()
            # YOLO ê¸°ë³¸ í´ë˜ìŠ¤ + ì‚¬ìš©ì ì •ì˜ CLIP ë¼ë²¨ë“¤
            extended_classes = base_classes.copy()
            extended_classes.extend(self.query_labels)
            return extended_classes
        return self.query_labels or ["custom_detection"]
    
    def set_confidence_threshold(self, threshold: float) -> None:
        """ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •"""
        if self.yolo_detector:
            self.yolo_detector.set_confidence_threshold(threshold)
    
    def set_defect_threshold(self, threshold: float) -> None:
        """CLIP defect ì„ê³„ê°’ ì„¤ì •"""
        self.defect_threshold = threshold
        print(f"ğŸ”§ Updated CLIP defect threshold to: {threshold}")
    
    def set_custom_queries(self, queries: List[str], labels: List[str] = None) -> None:
        """ì‚¬ìš©ì ì •ì˜ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¤ì •"""
        self.custom_queries = queries
        self.defect_queries = self.custom_queries  # ë³„ì¹­ ì—…ë°ì´íŠ¸
        self.query_labels = labels or self._generate_query_labels()
        print(f"ğŸ”§ Updated custom queries: {len(queries)} total")
        for i, (query, label) in enumerate(zip(self.custom_queries, self.query_labels)):
            print(f"   {i+1}. [{label}] {query}")
    
    def update_from_ui_config(self, defect_queries: List[str], defect_threshold: float) -> None:
        """UIì—ì„œ ì „ë‹¬ëœ ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸"""
        print(f"ğŸ”§ Updating YOLO+CLIP from UI config...")
        print(f"   Queries: {len(defect_queries)}")
        print(f"   Threshold: {defect_threshold}")
        
        self.custom_queries = defect_queries
        self.defect_queries = self.custom_queries  # ë³„ì¹­ ì—…ë°ì´íŠ¸
        self.defect_threshold = defect_threshold
        self.query_labels = self._generate_query_labels()
        
        print("ğŸ“ Updated queries and labels:")
        for i, (query, label) in enumerate(zip(self.custom_queries, self.query_labels)):
            print(f"   {i+1}. [{label}] {query}")
    
    def get_analysis_summary(self) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return {
            'defect_threshold': self.defect_threshold,
            'total_queries': len(self.custom_queries),
            'defect_queries_count': 6,  # ì²˜ìŒ 6ê°œê°€ ë¶ˆëŸ‰ ì¿¼ë¦¬
            'normal_queries_count': len(self.custom_queries) - 6,
            'queries': self.custom_queries,
            'device': str(self.device)
        }
    
    def is_loaded(self) -> bool:
        """í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸° ë¡œë“œ ìƒíƒœ í™•ì¸"""
        yolo_loaded = self.yolo_detector is not None and self.yolo_detector.is_loaded()
        clip_loaded = self.clip_model is not None
        return yolo_loaded and clip_loaded
    
    def get_model_info(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        base_info = super().get_model_info()
        base_info.update({
            'hybrid_type': 'yolo_clip',
            'yolo_loaded': self.yolo_detector is not None and self.yolo_detector.is_loaded(),
            'clip_loaded': self.clip_model is not None,
            'defect_threshold': self.defect_threshold,
            'defect_queries': self.custom_queries
        })
        return base_info
