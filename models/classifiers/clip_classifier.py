"""
CLIP ë¶„ë¥˜ê¸° êµ¬í˜„ (ë¯¸ëž˜ í™•ìž¥ìš©)
"""
import torch
import numpy as np
import cv2
from PIL import Image
from typing import List, Dict, Any, Optional
from ..base.base_classifier import BaseClassifier
from config.settings import CLIP_MODEL_NAME

class CLIPClassifier(BaseClassifier):
    """CLIP ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ë¥˜ê¸° (ë¯¸ëž˜ êµ¬í˜„ìš©)"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.model_name = kwargs.get('model_name', CLIP_MODEL_NAME)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.text_queries = kwargs.get('text_queries', [])
        
    def load_model(self) -> bool:
        """CLIP ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ðŸ“¥ Loading CLIP classifier: {self.model_name}")
            # TODO: CLIP ëª¨ë¸ ë¡œë“œ êµ¬í˜„
            # from transformers import CLIPProcessor, CLIPModel
            # self.processor = CLIPProcessor.from_pretrained(self.model_name)
            # self.model = CLIPModel.from_pretrained(self.model_name)
            # self.model = self.model.to(self.device)
            
            print("âš ï¸  CLIP classifier is not yet implemented")
            return False
        except Exception as e:
            print(f"âŒ Failed to load CLIP classifier: {e}")
            return False
    
    def extract_features(self, frame: np.ndarray, bbox: List[int]) -> Optional[np.ndarray]:
        """CLIPìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ"""
        if not self.is_loaded():
            return None
            
        try:
            x, y, w, h = bbox
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ í¬ë¡­
            cropped = frame[y:y+h, x:x+w]
            if cropped.size == 0:
                return None
            
            # BGRì„ RGBë¡œ ë³€í™˜
            cropped_rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(cropped_rgb)
            
            # TODO: CLIP íŠ¹ì§• ì¶”ì¶œ êµ¬í˜„
            # inputs = self.processor(images=pil_image, return_tensors="pt")
            # inputs = {k: v.to(self.device) for k, v in inputs.items()}
            # 
            # with torch.no_grad():
            #     image_features = self.model.get_image_features(**inputs)
            #     return image_features.cpu().numpy()
            
            return None
            
        except Exception as e:
            print(f"CLIP feature extraction error: {e}")
            return None
    
    def classify_features(self, features: np.ndarray, base_class: str) -> str:
        """CLIP íŠ¹ì§•ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜"""
        if not self.is_loaded() or features is None:
            return base_class
        
        try:
            # TODO: í…ìŠ¤íŠ¸ ì¿¼ë¦¬ì™€ ì´ë¯¸ì§€ íŠ¹ì§• ê°„ ìœ ì‚¬ë„ ê³„ì‚°
            # text_inputs = self.processor(text=self.text_queries, return_tensors="pt", padding=True)
            # text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}
            # 
            # with torch.no_grad():
            #     text_features = self.model.get_text_features(**text_inputs)
            #     similarities = torch.cosine_similarity(features, text_features)
            #     best_idx = similarities.argmax().item()
            #     
            #     if similarities[best_idx] > self.similarity_threshold:
            #         return self.text_queries[best_idx]
            
            return base_class
            
        except Exception as e:
            print(f"CLIP classification error: {e}")
            return base_class
    
    def set_text_queries(self, queries: List[str]) -> None:
        """ë¶„ë¥˜ìš© í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¤ì •"""
        self.text_queries = queries
    
    def get_model_info(self) -> Dict[str, Any]:
        """CLIP ë¶„ë¥˜ê¸° ì •ë³´ ë°˜í™˜"""
        base_info = super().get_model_info()
        base_info.update({
            'model_name': self.model_name,
            'text_queries': self.text_queries,
            'num_queries': len(self.text_queries)
        })
        return base_info
